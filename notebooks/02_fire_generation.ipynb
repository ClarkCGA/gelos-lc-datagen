{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5f12a-bb8e-4846-b696-1bd19f31c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e11f5-544d-414b-92ea-26baa6cb2cc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pystac_client\n",
    "import pystac\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3 import Retry\n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "import planetary_computer\n",
    "\n",
    "import dask.distributed\n",
    "import numpy as np\n",
    "import rioxarray\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from src.utils import search_s2_scenes, search_lc_scene, stack_s2_data, stack_lc_data, unique_class, missing_values, gen_chips\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546b8a0-5699-4a1c-8d08-1fa525b08ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4de254-75ae-4e58-ae3e-422ff6a4f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305e5b8-812e-4fde-80cd-3d51cc3979ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3988ac7-764b-4f5f-af05-f70d2a12a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_gdf = gpd.read_file(\"data/fires/final_non_intersecting_wildfires_2015_2023.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9416bce-b0b1-4cab-8407-1d564b84b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_gdf = aoi_gdf[aoi_gdf[\"initialdat\"] > datetime(2023, 1, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e8dae-3733-4d16-bc15-e6f7214f7de6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster()#(n_workers=8, threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b90881-1023-49bd-89bd-9f764872f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry = Retry(\n",
    "    total=10, backoff_factor=1, status_forcelist=[502, 503, 504], allowed_methods=None\n",
    ")\n",
    "stac_api_io = StacApiIO(max_retries=retry)\n",
    "\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    "    stac_io=stac_api_io\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11988702-1648-400d-894c-796738f3028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chips(s2_stack, lc_stack, epsg, sample_size, chip_size, global_index, metadata_df):\n",
    "    \n",
    "    # try:\n",
    "    #     lc_stack = lc_stack.compute()\n",
    "    # except:\n",
    "    #     print(\"skipping the AOI for no LC data\")\n",
    "    #     return global_index, metadata_df\n",
    "    \n",
    "    # lc_uniqueness = lc_stack.coarsen(x = sample_size,\n",
    "    #                                  y = sample_size,\n",
    "    #                                  boundary = \"trim\"\n",
    "    #                                 ).reduce(unique_class)\n",
    "    # lc_uniqueness[0, :] = False\n",
    "    # lc_uniqueness[-1, :] = False\n",
    "    # lc_uniqueness[:, 0] = False\n",
    "    # lc_uniqueness[:, -1] = False\n",
    "\n",
    "    # ys, xs = np.where(lc_uniqueness)\n",
    "    # print(\"Loading s2_stack\")\n",
    "    \n",
    "    try:\n",
    "        s2_stack = s2_stack.compute()\n",
    "    except:\n",
    "        print(\"skipping the AOI for no S2 data\")\n",
    "        return global_index, metadata_df\n",
    "    \n",
    "    \n",
    "    for index in range(0, len(ys)):\n",
    "        y = ys[index]\n",
    "        x = xs[index]\n",
    "    \n",
    "            \n",
    "        x_coords = slice((x) * sample_size - int((chip_size - sample_size)/2), (x + 1) * sample_size + int((chip_size - sample_size)/2))\n",
    "        y_coords = slice((y) * sample_size - int((chip_size - sample_size)/2), (y + 1) * sample_size + int((chip_size - sample_size)/2))    \n",
    "        \n",
    "        s2_array = s2_stack.isel(x = x_coords, y = y_coords)\n",
    "        s2_array.rio.write_crs(f\"epsg:{epsg}\", inplace=True)\n",
    "        s2_array = s2_array.where((s2_array.x >= s2_stack.x[(x) * sample_size]) &\n",
    "                                  (s2_array.x < s2_stack.x[(x + 1) * sample_size]) & \n",
    "                                  (s2_array.y <= s2_stack.y[(y) * sample_size]) &\n",
    "                                  (s2_array.y > s2_stack.y[(y + 1) * sample_size])\n",
    "                                 )\n",
    "        \n",
    "        if missing_values(s2_array, chip_size, sample_size):\n",
    "            # print(f\"Skipping chip at index {index}\")\n",
    "            continue        \n",
    "        \n",
    "        s2_array = s2_array.fillna(-999)\n",
    "        s2_array = s2_array.rio.write_nodata(-999)\n",
    "        s2_array = s2_array.astype(np.dtype(np.int16))\n",
    "        s2_array = s2_array.rename(\"s2\")\n",
    "\n",
    "        dnbr = nbr_threshold(s2_array)\n",
    "        print(dnbr)\n",
    "\n",
    "                \n",
    "        lc_array = lc_stack.isel(x = x_coords, y = y_coords)\n",
    "        lc_array.rio.write_crs(f\"epsg:{epsg}\", inplace=True)\n",
    "        lc_array = lc_array.where((lc_array.x >= lc_stack.x[(x) * sample_size]) &\n",
    "                                  (lc_array.x < lc_stack.x[(x + 1) * sample_size]) & \n",
    "                                  (lc_array.y <= lc_stack.y[(y) * sample_size] ) &\n",
    "                                  (lc_array.y > lc_stack.y[(y + 1) * sample_size])\n",
    "                                 )\n",
    "        \n",
    "        if missing_values(lc_array, chip_size, sample_size):\n",
    "            # print(f\"Skipping chip at index {index}\")\n",
    "            continue\n",
    "\n",
    "        if (np.isin(lc_array, [255, 130, 133])).any():\n",
    "            raise ValueError('Wrong LC value')\n",
    "        \n",
    "        lc_array = lc_array.fillna(0)\n",
    "        lc_array = lc_array.rio.write_nodata(0)\n",
    "        lc_array = lc_array.astype(np.dtype(np.int8))\n",
    "        lc_array = lc_array.rename(\"lc\")\n",
    "        if (np.isin(lc_array, [7])).any():\n",
    "            gen_status, dts = gen_chips(s2_array, lc_array, global_index)\n",
    "            if gen_status:\n",
    "                metadata_df = pd.concat([pd.DataFrame([[global_index,\n",
    "                                                        dts,\n",
    "                                                        np.unique(lc_array)[1],\n",
    "                                                        s2_stack.x[(x) * sample_size + int(sample_size / 2)].data,\n",
    "                                                        s2_stack.y[(y) * sample_size + int(sample_size / 2)].data,\n",
    "                                                        epsg]\n",
    "                                                      ],\n",
    "                                                      columns=metadata_df.columns\n",
    "                                                     ),\n",
    "                                         metadata_df],\n",
    "                                        ignore_index=True\n",
    "                                       )\n",
    "                global_index += 1\n",
    "    \n",
    "    return global_index, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531bbc1-f8ba-40b3-b748-0c8d739cda05",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_index = 0\n",
    "metadata_df = pd.DataFrame(columns=[\"chip_id\", \"dates\", \"lc\", \"x_center\", \"y_center\", \"epsg\"])\n",
    "# metadata_df = pd.read_csv(\"../data/metadata_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67481a-b845-4749-b4b3-a887e8663de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_fire_gdf = fire_gdf[fire_gdf[\"area_ha\"]>2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87359616-18ae-4d85-b4f9-25a75dfe7b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_ranges(row):\n",
    "    pre_start_date = row[\"initialdat\"]-timedelta(days=91)\n",
    "    pre_end_date = row[\"initialdat\"]-timedelta(days=1)\n",
    "\n",
    "    post_start_date = row[\"finaldate\"]+timedelta(days=1)\n",
    "    post_end_date = row[\"finaldate\"]+timedelta(days=91)\n",
    "\n",
    "    pre_dates = f\"{str(pre_start_date).split(\" \")[0]}/{str(pre_end_date).split(\" \")[0]}\"\n",
    "    post_dates = f\"{str(post_start_date).split(\" \")[0]}/{str(post_end_date).split(\" \")[0]}\"\n",
    "    \n",
    "    return [pre_dates, post_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf21f8d-f903-46f5-994d-98c7175c348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import json\n",
    "from shapely.geometry import Polygon, mapping, box\n",
    "def nbr_threshold(s2_array):\n",
    "    dnbr = cal_dnbr(s2_array)\n",
    "    nbr_threshold = xr.where(dnbr > 0.3, 1, 0)\n",
    "\n",
    "    return nbr_threshold.mean(skipna=True)\n",
    "\n",
    "def cal_dnbr(s2_array):\n",
    "    nbr_pre = nbr(s2_array.isel(time=0))\n",
    "    nbr_post = nbr(s2_array.isel(time=1))\n",
    "    dnbr = nbr_pre - nbr_post\n",
    "    \n",
    "    return dnbr\n",
    "\n",
    "\n",
    "def nbr(data):\n",
    "    nbr = (data.sel(band=\"B08\") - data.sel(band=\"B12\")) / (data.sel(band=\"B08\") + data.sel(band=\"B12\"))\n",
    "\n",
    "    return nbr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae971c5-2245-4ac9-88c6-a65e4d453427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, aoi in large_fire_gdf.iterrows():\n",
    "    print(f\"\\nProcessing AOI at index {index}\")\n",
    "    \n",
    "    aoi_bounds = aoi['geometry'].bounds\n",
    "    s2_items = pystac.item_collection.ItemCollection([])\n",
    "    date_ranges = get_date_ranges(aoi)\n",
    "    for date_range in date_ranges:        \n",
    "        s2_items_season = search_s2_scenes(aoi, date_range, catalog, config)\n",
    "        s2_items += s2_items_season\n",
    "\n",
    "    if len(s2_items)<2:\n",
    "        print(f\"Missing Sentinel-2 scenes for AOI {aoi_bounds}\")\n",
    "        continue\n",
    "        \n",
    "\n",
    "    s2_stack = stack_s2_data(s2_items, config)\n",
    "    if s2_stack is None:\n",
    "        print(f\"Failed to stack Sentinel-2 bands for AOI {aoi_bounds}\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        epsg = s2_items[0].properties[\"proj:epsg\"]\n",
    "    except:\n",
    "        epsg = int(s2_items[0].properties[\"proj:code\"].split(\":\")[-1])\n",
    "\n",
    "    import stackstac \n",
    "    from src.utils import mask_cloudy_pixels\n",
    "    clipping_geom = aoi[\"geometry\"]\n",
    "\n",
    "    try:\n",
    "        s2_stack = stackstac.stack(\n",
    "            s2_items,\n",
    "            assets=config[\"sentinel_2\"][\"bands\"],\n",
    "            epsg=epsg,\n",
    "            resolution=config[\"sentinel_2\"][\"resolution\"],\n",
    "            fill_value=np.nan,\n",
    "            bounds_latlon = clipping_geom.bounds\n",
    "        )\n",
    "        #s2_stack = s2_stack.chunk(chunks={\"band\": len(config[\"sentinel_2\"][\"bands\"]), \"x\": -1, \"y\": \"auto\"})\n",
    "        s2_stack = mask_cloudy_pixels(s2_stack)\n",
    "        s2_stack = s2_stack.drop_sel(band=\"SCL\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Sentinel-2 data: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        s2_stack = s2_stack.compute()\n",
    "    except:\n",
    "        print(\"skipping the AOI for no S2 data\")\n",
    "\n",
    "    dnbr_scene = nbr_threshold(s2_stack)\n",
    "    print(f\"dnbr mean {dnbr_scene.data}\")\n",
    "    if dnbr_scene.data > 0.25:\n",
    "        for dt in s2_stack.time.values:\n",
    "            ts = pd.to_datetime(str(dt)) \n",
    "            s2_path = f\"/home/benchuser/s2_{index:06}_{ts.strftime('%Y%m%d')}.tif\"\n",
    "            s2_stack.sel(time = dt).squeeze().rio.to_raster(s2_path)\n",
    "\n",
    "        # Define a Shapely Polygon\n",
    "        polygon = aoi[\"geometry\"]\n",
    "        \n",
    "        # Convert to GeoJSON format\n",
    "        geojson_data = {\n",
    "            \"type\": \"Feature\",\n",
    "            \"geometry\": mapping(polygon),\n",
    "            \"properties\": {}  # You can add attributes here if needed\n",
    "        }\n",
    "        \n",
    "        # Save to a .geojson file\n",
    "        with open(f\"/home/benchuser/{index:06}.geojson\", \"w\") as f:\n",
    "            json.dump(geojson_data, f, indent=4)\n",
    "\n",
    "\n",
    "    # lc_items = search_lc_scene(s2_items[0].bbox, catalog, config)\n",
    "    # if not lc_items:\n",
    "    #     print(f\"No Land Cover data found for AOI {aoi_bounds}\")\n",
    "    #     continue\n",
    "    \n",
    "    # lc_stack = stack_lc_data(lc_items, s2_stack.rio.crs.to_epsg(), s2_items[0].bbox, config)\n",
    "    # if lc_stack is None:\n",
    "    #     print(f\"Failed to stack Land Cover data for AOI {aoi_bounds} and date range {date_range}\")\n",
    "    #     continue\n",
    "    #break\n",
    "    # global_index, metadata_df = process_chips(s2_stack,\n",
    "    #                                           lc_stack,\n",
    "    #                                           epsg,\n",
    "    #                                           config[\"chips\"][\"sample_size\"],\n",
    "    #                                           config[\"chips\"][\"chip_size\"],\n",
    "    #                                           global_index,\n",
    "    #                                           metadata_df)\n",
    "    # metadata_df.to_csv('/home/benchuser/data/metadata_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8c224c-cb01-446c-9134-3e3566ca616b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
