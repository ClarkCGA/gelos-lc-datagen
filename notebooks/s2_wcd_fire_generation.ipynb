{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666e0ed1",
   "metadata": {},
   "source": [
    "##### Pipeline for Generating Time Series Burn scar datasets for GFM Bench (outside of USA) \n",
    "In this pipeline, we use the `pre and post fire images   to define a \"year\" for generating the time series, generate 4 images during that event year, and use the same range of months in previous years to sample for control years. We ensure that the event year has a sample after the date of the post fire image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb351749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "os.environ[\"CPL_VSIL_CURL_NUM_CONNECTIONS\"] = \"20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4988df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "import pystac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import stackstac \n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "from urllib3 import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "import dask.distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "from rasterio.features import rasterize\n",
    "from shapely.geometry import mapping\n",
    "from shapely.wkt import loads\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.ndimage import uniform_filter\n",
    "from shapely.geometry import shape\n",
    "from src.utils import (search_s2_scenes, \n",
    "                       stack_s2_data, \n",
    "                       unique_class, \n",
    "                       missing_values, \n",
    "                       gen_chips, \n",
    "                       mask_cloudy_pixels\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f8cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3982c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>event_type</th>\n",
       "      <th>date</th>\n",
       "      <th>path</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Almonaster</td>\n",
       "      <td>pre</td>\n",
       "      <td>2022-07-12</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Almonaster/img1_c...</td>\n",
       "      <td>MULTIPOLYGON (((-1.9465144286373106 41.3722766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Almonaster</td>\n",
       "      <td>post</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Almonaster/img2_c...</td>\n",
       "      <td>MULTIPOLYGON (((-1.9465144286373106 41.3722766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Almonaster</td>\n",
       "      <td>mask</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Almonaster/cm/cm.tif</td>\n",
       "      <td>MULTIPOLYGON (((-1.9465144286373106 41.3722766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attica</td>\n",
       "      <td>pre</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Attica/img1_cropp...</td>\n",
       "      <td>MULTIPOLYGON (((-0.7064004420359428 38.0931080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attica</td>\n",
       "      <td>post</td>\n",
       "      <td>2021-08-28</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Attica/img2_cropp...</td>\n",
       "      <td>MULTIPOLYGON (((-0.7064004420359428 38.0931080...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     location event_type        date  \\\n",
       "0  Almonaster        pre  2022-07-12   \n",
       "1  Almonaster       post  2022-07-27   \n",
       "2  Almonaster       mask         NaN   \n",
       "3      Attica        pre  2021-08-16   \n",
       "4      Attica       post  2021-08-28   \n",
       "\n",
       "                                                path  \\\n",
       "0  /workspace/Rufai/data/S2-WCD/Almonaster/img1_c...   \n",
       "1  /workspace/Rufai/data/S2-WCD/Almonaster/img2_c...   \n",
       "2  /workspace/Rufai/data/S2-WCD/Almonaster/cm/cm.tif   \n",
       "3  /workspace/Rufai/data/S2-WCD/Attica/img1_cropp...   \n",
       "4  /workspace/Rufai/data/S2-WCD/Attica/img2_cropp...   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-1.9465144286373106 41.3722766...  \n",
       "1  MULTIPOLYGON (((-1.9465144286373106 41.3722766...  \n",
       "2  MULTIPOLYGON (((-1.9465144286373106 41.3722766...  \n",
       "3  MULTIPOLYGON (((-0.7064004420359428 38.0931080...  \n",
       "4  MULTIPOLYGON (((-0.7064004420359428 38.0931080...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fire events data\n",
    "fire_df = pd.read_csv(\"data/s2_wcd_fires.csv\")\n",
    "fire_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a1bae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fire_mask_path(fire_df):\n",
    "    df_filtered = fire_df[fire_df[\"event_type\"].isin([\"pre\", \"post\"])]\n",
    "    summary = (\n",
    "        df_filtered\n",
    "        .groupby(\"location\")\n",
    "        .agg(\n",
    "            geometry=(\"geometry\", \"first\"),\n",
    "            pre_date=(\"date\", lambda x: sorted(x[df_filtered.loc[x.index, \"event_type\"] == \"pre\"])[0] if any(df_filtered.loc[x.index, \"event_type\"] == \"pre\") else None),\n",
    "            post_date=(\"date\", lambda x: sorted(x[df_filtered.loc[x.index, \"event_type\"] == \"post\"])[0] if any(df_filtered.loc[x.index, \"event_type\"] == \"post\") else None),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    if isinstance(summary[\"geometry\"].iloc[0], str):\n",
    "        summary[\"geometry\"] = summary[\"geometry\"].apply(loads)\n",
    "    summary_gdf = gpd.GeoDataFrame(summary, geometry=summary[\"geometry\"], crs=\"EPSG:4326\")\n",
    "    summary_gdf[\"pre_date\"] = pd.to_datetime(summary_gdf[\"pre_date\"])\n",
    "    summary_gdf[\"post_date\"] = pd.to_datetime(summary_gdf[\"post_date\"])\n",
    "\n",
    "    mask_paths = (\n",
    "        fire_df[fire_df[\"event_type\"] == \"mask\"]\n",
    "        .groupby(\"location\")[\"path\"]\n",
    "        .first()  #one fire mask per location\n",
    "        .reset_index()\n",
    "        .rename(columns={\"path\": \"mask_path\"})\n",
    "    )\n",
    "    summary_gdf = summary_gdf.merge(mask_paths, on=\"location\", how=\"left\")\n",
    "    return summary_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe1b8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>geometry</th>\n",
       "      <th>pre_date</th>\n",
       "      <th>post_date</th>\n",
       "      <th>mask_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Almonaster</td>\n",
       "      <td>MULTIPOLYGON (((-1.94651 41.37228, -1.94652 41...</td>\n",
       "      <td>2022-07-12</td>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Almonaster/cm/cm.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Attica</td>\n",
       "      <td>MULTIPOLYGON (((-0.7064 38.09311, -0.7064 38.0...</td>\n",
       "      <td>2021-08-16</td>\n",
       "      <td>2021-08-28</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Attica/cm/cm.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Australia_1</td>\n",
       "      <td>POLYGON ((-4.47264 58.56369, -4.47263 58.5636,...</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-02-20</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Australia_1/cm/cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Australia_2</td>\n",
       "      <td>MULTIPOLYGON (((-4.49479 58.50658, -4.49513 58...</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-02-20</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Australia_2/cm/cm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bejis</td>\n",
       "      <td>MULTIPOLYGON (((-0.71135 39.88491, -0.71147 39...</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>/workspace/Rufai/data/S2-WCD/Bejis/cm/cm.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      location                                           geometry   pre_date  \\\n",
       "0   Almonaster  MULTIPOLYGON (((-1.94651 41.37228, -1.94652 41... 2022-07-12   \n",
       "1       Attica  MULTIPOLYGON (((-0.7064 38.09311, -0.7064 38.0... 2021-08-16   \n",
       "2  Australia_1  POLYGON ((-4.47264 58.56369, -4.47263 58.5636,... 2021-01-31   \n",
       "3  Australia_2  MULTIPOLYGON (((-4.49479 58.50658, -4.49513 58... 2021-01-31   \n",
       "4        Bejis  MULTIPOLYGON (((-0.71135 39.88491, -0.71147 39... 2022-08-08   \n",
       "\n",
       "   post_date                                          mask_path  \n",
       "0 2022-07-27  /workspace/Rufai/data/S2-WCD/Almonaster/cm/cm.tif  \n",
       "1 2021-08-28      /workspace/Rufai/data/S2-WCD/Attica/cm/cm.tif  \n",
       "2 2021-02-20  /workspace/Rufai/data/S2-WCD/Australia_1/cm/cm...  \n",
       "3 2021-02-20  /workspace/Rufai/data/S2-WCD/Australia_2/cm/cm...  \n",
       "4 2022-08-23       /workspace/Rufai/data/S2-WCD/Bejis/cm/cm.tif  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_gdf = get_fire_mask_path(fire_df)\n",
    "summary_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d235e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fire events: 41\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of fire events:\", len(summary_gdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee6c75",
   "metadata": {},
   "source": [
    "### Load datasets from STAC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7b6ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    }
   ],
   "source": [
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55071abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry = Retry(\n",
    "    total=10, backoff_factor=1, status_forcelist=[502, 503, 504], allowed_methods=None\n",
    ")\n",
    "stac_api_io = StacApiIO(max_retries=retry)\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    "    stac_io=stac_api_io\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40b7b948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 4 PER event year (calendar not seasons)\n",
    "# 4 for for each control year (depending on the availability of Sentinel-2 data)\n",
    "def get_date_ranges(row, n_control_years: int = 7):\n",
    "    post_date   = pd.to_datetime(row[\"post_date\"])\n",
    "    event_year  = post_date.year\n",
    "\n",
    "    # Quarter breakpoints\n",
    "    quarters = [(1, 3), (4, 6), (7, 9), (10, 12)]\n",
    "\n",
    "    def quarter_window(year, start_m, end_m):\n",
    "        \"\"\"Return a 'YYYY-MM-DD/YYYY-MM-DD' string for a single quarter.\"\"\"\n",
    "        start = f\"{year}-{start_m:02d}-01\"\n",
    "        end_day = calendar.monthrange(year, end_m)[1]\n",
    "        end   = f\"{year}-{end_m:02d}-{end_day:02d}\"\n",
    "        return f\"{start}/{end}\"\n",
    "\n",
    "    # Event-year ranges (always 4)\n",
    "    event_ranges = [\n",
    "        quarter_window(event_year, sm, em) for sm, em in quarters\n",
    "    ]\n",
    "\n",
    "    # Control-year ranges\n",
    "    control_ranges = []\n",
    "    for delta in range(1, n_control_years + 1):\n",
    "        cy = event_year - delta\n",
    "        control_ranges.append([\n",
    "            quarter_window(cy, sm, em) for sm, em in quarters\n",
    "        ])\n",
    "    return event_ranges, control_ranges\n",
    "\n",
    "def rasterize_aoi(aoi, s2_stack):\n",
    "    \"\"\"\n",
    "    Rasterize the AOI polygon into a burn mask\n",
    "    \"\"\"\n",
    "    aoi_gdf = gpd.GeoDataFrame(\n",
    "        {\"geometry\": [shape(aoi['geometry'])]},\n",
    "        crs=\"EPSG:4326\"\n",
    "    )\n",
    "    \n",
    "    aoi_proj = aoi_gdf.to_crs(s2_stack.rio.crs)\n",
    "    \n",
    "    burn_mask = rasterize(\n",
    "        [(mapping(aoi_proj['geometry'].iloc[0]), 1)],\n",
    "        out_shape=(s2_stack.sizes['y'], s2_stack.sizes['x']),\n",
    "        transform=s2_stack.rio.transform(),\n",
    "        fill=0,\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    \n",
    "    burn_mask_da = xr.DataArray(\n",
    "        burn_mask,\n",
    "        coords={\"y\": s2_stack[\"y\"], \"x\": s2_stack[\"x\"]},\n",
    "        dims=(\"y\", \"x\")\n",
    "    )\n",
    "    return burn_mask_da\n",
    "\n",
    "\n",
    "# def crop_burn_window(s2_stack, burn_mask, config):\n",
    "#     window_size = config['chips']['chip_size']\n",
    "\n",
    "#     # Apply uniform filter (mean filter), then scale to get sum\n",
    "#     burn_mean = uniform_filter(burn_mask.values.astype(float), size=window_size, mode='constant', cval=0.0)\n",
    "#     burn_sum = burn_mean * (window_size ** 2)\n",
    "    \n",
    "#     # Find maximum sum\n",
    "#     max_idx = np.unravel_index(np.argmax(burn_sum), burn_sum.shape)\n",
    "#     y_idx, x_idx = max_idx\n",
    "    \n",
    "#     # Calculate start indices\n",
    "#     y_start = max(y_idx - window_size // 2, 0)\n",
    "#     x_start = max(x_idx - window_size // 2, 0)\n",
    "        \n",
    "#     cropped_stack = s2_stack.isel(\n",
    "#         y=slice(y_start, y_start + window_size),\n",
    "#         x=slice(x_start, x_start + window_size)\n",
    "#     )\n",
    "    \n",
    "#     return cropped_stack\n",
    "\n",
    "def crop_burn_windows(s2_stack, burn_mask, chip_size=224, pct_threshold=0.30):\n",
    "    \"\"\"\n",
    "    Return *all* windows of size `chip_size`×`chip_size` whose burn-pixel\n",
    "    fraction is ≥ `pct_threshold`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[xr.DataArray]  each element is a cropped stack\n",
    "    \"\"\"\n",
    "    step = chip_size        # non-overlap; set <chip_size for stride/overlap\n",
    "    ny, nx = burn_mask.shape\n",
    "    chips = []\n",
    "\n",
    "    for y0 in range(0, ny - chip_size + 1, step):\n",
    "        for x0 in range(0, nx - chip_size + 1, step):\n",
    "            window = burn_mask[y0:y0+chip_size, x0:x0+chip_size]\n",
    "            frac   = window.mean()        # fraction of burn pixels in the window\n",
    "            if frac >= pct_threshold:     # keep only “burn-rich” windows\n",
    "                chips.append(\n",
    "                    s2_stack.isel(\n",
    "                        y=slice(y0, y0+chip_size),\n",
    "                        x=slice(x0, x0+chip_size),\n",
    "                        drop=False\n",
    "                    )\n",
    "                )\n",
    "    return chips\n",
    "\n",
    "def harmonize_to_old(data):\n",
    "    \"\"\"\n",
    "    Harmonize new Sentinel-2 data to the old baseline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: xarray.DataArray\n",
    "        A DataArray with four dimensions: time, band, y, x\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    harmonized: xarray.DataArray\n",
    "        A DataArray with all values harmonized to the old\n",
    "        processing baseline.\n",
    "    \"\"\"\n",
    "    if \"time\" not in data.dims:\n",
    "        # static composite → nothing to do\n",
    "        return data\n",
    "    if \"time\" not in data.dims:\n",
    "        if \"time\" in data:\n",
    "            data = data.expand_dims(\"time\")  # convert scalar to 1D dimension\n",
    "        else:\n",
    "            raise ValueError(\"Variable 'time' not found in dataset.\")\n",
    "            \n",
    "    data = data.set_index(time=\"time\")\n",
    "\n",
    "    cutoff = datetime(2022, 1, 25)\n",
    "    offset = 1000\n",
    "    bands = [\n",
    "        \"B01\",\n",
    "        \"B02\",\n",
    "        \"B03\",\n",
    "        \"B04\",\n",
    "        \"B05\",\n",
    "        \"B06\",\n",
    "        \"B07\",\n",
    "        \"B08\",\n",
    "        \"B8A\",\n",
    "        \"B09\",\n",
    "        \"B10\",\n",
    "        \"B11\",\n",
    "        \"B12\",\n",
    "    ]\n",
    "\n",
    "    old = data.sel(time=slice(cutoff))\n",
    "\n",
    "    to_process = list(set(bands) & set(data.band.data.tolist()))\n",
    "    new = data.sel(time=slice(cutoff, None)).drop_sel(band=to_process)\n",
    "\n",
    "    new_harmonized = data.sel(time=slice(cutoff, None), band=to_process).clip(offset)\n",
    "    new_harmonized -= offset\n",
    "\n",
    "    new = xr.concat([new, new_harmonized], \"band\").sel(band=data.band.data.tolist())\n",
    "    \n",
    "    return xr.concat([old, new], dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e8d969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fire_chips(s2_stack, aoi, config, time_series_type, epsg, index, metadata_df):\n",
    "    saved_any = False\n",
    "    try:\n",
    "        s2_stack = s2_stack.compute()\n",
    "    except:\n",
    "        print(\"skipping the AOI for no S2 data\")\n",
    "    if s2_stack.shape[2] != 224 or s2_stack.shape[3] != 224:\n",
    "        print(f\"Skipping chip ID {index} for mismatch dimensions\")\n",
    "        return False, metadata_df \n",
    "    \n",
    "    for crop_idx, s2_stack_cropped in enumerate(s2_stack):\n",
    "        if \"time\" not in s2_stack_cropped.dims:\n",
    "            if \"time\" in s2_stack_cropped.coords:\n",
    "                s2_stack_cropped = s2_stack_cropped.expand_dims(\"time\")\n",
    "            else:\n",
    "                print(f\"Skipping chip ID {index} — no time dimension present\")\n",
    "                continue\n",
    "        if missing_values(s2_stack_cropped, config['chips']['chip_size'], config['chips']['chip_size']):\n",
    "            print(f\"Skipping chip ID {index} for missing values\")\n",
    "            continue      \n",
    "                \n",
    "        s2_stack_cropped = harmonize_to_old(s2_stack_cropped)\n",
    "    \n",
    "        s2_stack_cropped = s2_stack_cropped.fillna(-999)\n",
    "        s2_stack_cropped = s2_stack_cropped.rio.write_nodata(-999)\n",
    "        s2_stack_cropped = s2_stack_cropped.astype(np.dtype(np.int16))\n",
    "        s2_stack_cropped = s2_stack_cropped.rename(\"s2\")\n",
    "\n",
    "        if time_series_type == \"event\":\n",
    "            for dt in s2_stack_cropped.time.values:\n",
    "                print(f\"Processing chip ID {index} for event date {dt}\")\n",
    "                ts = pd.to_datetime(str(dt)) \n",
    "                s2_path = f\"data/fire_data/s2_{index:06}_e_{ts.strftime('%Y%m%d')}.tif\"\n",
    "                if os.path.exists(s2_path):\n",
    "                    print(f\"Skipping chip ID {index}_{crop_idx} for chip {index} date: {ts.strftime('%Y%m%d')} — file already exists\")\n",
    "                    continue\n",
    "                print(f\"Saving chip ID {index}_{crop_idx} for chip {index} date: {ts.strftime('%Y%m%d')}\")\n",
    "                s2_stack_cropped.sel(time=dt).squeeze().rio.to_raster(s2_path)\n",
    "\n",
    "                s2_stack_cropped.sel(time = dt).squeeze().rio.to_raster(s2_path)\n",
    "\n",
    "                metadata_df = pd.concat([pd.DataFrame([[index,\n",
    "                                                        ts.strftime('%Y%m%d'),\n",
    "                                                        f\"{index:06}_e_{ts.strftime('%Y%m%d')}\",\n",
    "                                                        \"event\",\n",
    "                                                        s2_stack_cropped.x[int(len(s2_stack_cropped.x)/2)].data,\n",
    "                                                        s2_stack_cropped.y[int(len(s2_stack_cropped.y)/2)].data,\n",
    "                                                        epsg]\n",
    "                                                    ],\n",
    "                                                    columns=metadata_df.columns\n",
    "                                                    ),\n",
    "                                        metadata_df],\n",
    "                                        ignore_index=True\n",
    "                                    )\n",
    "                saved_any = True\n",
    "        else:\n",
    "            for dt in s2_stack_cropped.time.values:\n",
    "                print(f\"Processing chip ID {index} for control date {dt}\")\n",
    "            # dt = s2_stack_cropped.time.values[0]\n",
    "                ts = pd.to_datetime(str(dt)) \n",
    "                s2_path = f\"data/fire_data/s2_{index:06}_c_{ts.strftime('%Y%m%d')}.tif\"\n",
    "                if os.path.exists(s2_path):\n",
    "                    print(f\"Skipping chip ID {index}_{crop_idx} for chip {index} date: {ts.strftime('%Y%m%d')} — file already exists\")\n",
    "                    continue\n",
    "                print(f\"Saving chip ID {index}_{crop_idx} for chip {index} date: {ts.strftime('%Y%m%d')}\")\n",
    "                s2_stack_cropped.sel(time = dt).squeeze().rio.to_raster(s2_path)\n",
    "                metadata_df = pd.concat([pd.DataFrame([[index,\n",
    "                                                        ts.strftime('%Y%m%d'),\n",
    "                                                        f\"{index:06}_c_{ts.strftime('%Y%m%d')}\",\n",
    "                                                        \"control\",\n",
    "                                                        s2_stack_cropped.x[int(len(s2_stack_cropped.x)/2)].data,\n",
    "                                                        s2_stack_cropped.y[int(len(s2_stack_cropped.y)/2)].data,\n",
    "                                                        epsg]\n",
    "                                                    ],\n",
    "                                            columns=metadata_df.columns\n",
    "                                            ),\n",
    "                                metadata_df],\n",
    "                                ignore_index=True\n",
    "                            )\n",
    "                saved_any = True\n",
    "    return saved_any, metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0442003d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_df = pd.DataFrame(columns=[\"chip_id\", \"date\", \"sample_id\", \"type\", \"x_center\", \"y_center\", \"epsg\"])\n",
    "metadata_df = pd.read_csv(\"data/metadata_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5183c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_s2_scenes(aoi, date_range, catalog, config, best_one=True):\n",
    "    search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=aoi.geometry.bounds,\n",
    "        datetime=date_range,\n",
    "        query=[f\"s2:nodata_pixel_percentage<{config[\"sentinel_2\"][\"nodata_pixel_percentage\"]}\",\n",
    "                 f\"eo:cloud_cover<{config[\"sentinel_2\"][\"cloud_cover\"]}\"\n",
    "                ],\n",
    "        limit=None,                     \n",
    "    )\n",
    "    items = list(search.get_items())\n",
    "\n",
    "    if best_one and items:\n",
    "        items = [min(items, key=lambda it: it.properties[\"eo:cloud_cover\"])]\n",
    "\n",
    "    return pystac.ItemCollection(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0448fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, planetary_computer as pc, rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "import stackstac, dask.array as da\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "\n",
    "@retry(wait=wait_random_exponential(max=30), stop=stop_after_attempt(4))\n",
    "def _open_once(href):\n",
    "    # one quick probe to fail fast if token is stale\n",
    "    with rasterio.open(href):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf03d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing AOI at index 0\n",
      "Found 7 chips for AOI (-1.9472346869945791, 41.37056606450764, -1.8334312864604134, 41.43045545782507)\n",
      "Skipping chip ID 0_0 for missing values\n",
      "Processing chip ID 0_0 for event date 2022-05-28T10:56:19.024000000\n",
      "Skipping chip ID 0_0_1 for chip 0_0 date: 20220528 — file already exists\n",
      "Processing chip ID 0_0 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_0_2 for chip 0_0 date: 20220831\n",
      "Processing chip ID 0_0 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_0_3 for chip 0_0 date: 20221005\n",
      "Processing chip ID 0_1 for event date 2022-02-22T11:00:51.024000000\n",
      "Skipping chip ID 0_1_0 for chip 0_1 date: 20220222 — file already exists\n",
      "Processing chip ID 0_1 for event date 2022-05-28T10:56:19.024000000\n",
      "Saving chip ID 0_1_1 for chip 0_1 date: 20220528\n",
      "Processing chip ID 0_1 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_1_2 for chip 0_1 date: 20220831\n",
      "Processing chip ID 0_1 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_1_3 for chip 0_1 date: 20221005\n",
      "Processing chip ID 0_2 for event date 2022-02-22T11:00:51.024000000\n",
      "Skipping chip ID 0_2_0 for chip 0_2 date: 20220222 — file already exists\n",
      "Processing chip ID 0_2 for event date 2022-05-28T10:56:19.024000000\n",
      "Saving chip ID 0_2_1 for chip 0_2 date: 20220528\n",
      "Processing chip ID 0_2 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_2_2 for chip 0_2 date: 20220831\n",
      "Processing chip ID 0_2 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_2_3 for chip 0_2 date: 20221005\n",
      "Processing chip ID 0_3 for event date 2022-02-22T11:00:51.024000000\n",
      "Skipping chip ID 0_3_0 for chip 0_3 date: 20220222 — file already exists\n",
      "Processing chip ID 0_3 for event date 2022-05-28T10:56:19.024000000\n",
      "Saving chip ID 0_3_1 for chip 0_3 date: 20220528\n",
      "Processing chip ID 0_3 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_3_2 for chip 0_3 date: 20220831\n",
      "Processing chip ID 0_3 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_3_3 for chip 0_3 date: 20221005\n",
      "Skipping chip ID 0_4 for missing values\n",
      "Processing chip ID 0_4 for event date 2022-05-28T10:56:19.024000000\n",
      "Skipping chip ID 0_4_1 for chip 0_4 date: 20220528 — file already exists\n",
      "Processing chip ID 0_4 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_4_2 for chip 0_4 date: 20220831\n",
      "Processing chip ID 0_4 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_4_3 for chip 0_4 date: 20221005\n",
      "Processing chip ID 0_5 for event date 2022-02-22T11:00:51.024000000\n",
      "Skipping chip ID 0_5_0 for chip 0_5 date: 20220222 — file already exists\n",
      "Processing chip ID 0_5 for event date 2022-05-28T10:56:19.024000000\n",
      "Saving chip ID 0_5_1 for chip 0_5 date: 20220528\n",
      "Processing chip ID 0_5 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_5_2 for chip 0_5 date: 20220831\n",
      "Processing chip ID 0_5 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_5_3 for chip 0_5 date: 20221005\n",
      "Processing chip ID 0_6 for event date 2022-02-22T11:00:51.024000000\n",
      "Skipping chip ID 0_6_0 for chip 0_6 date: 20220222 — file already exists\n",
      "Processing chip ID 0_6 for event date 2022-05-28T10:56:19.024000000\n",
      "Saving chip ID 0_6_1 for chip 0_6 date: 20220528\n",
      "Processing chip ID 0_6 for event date 2022-08-31T10:56:31.024000000\n",
      "Saving chip ID 0_6_2 for chip 0_6 date: 20220831\n",
      "Processing chip ID 0_6 for event date 2022-10-05T10:58:19.024000000\n",
      "Saving chip ID 0_6_3 for chip 0_6 date: 20221005\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Missing Sentinel-2 scenes for AOI (-1.9472346869945791, 41.37056606450764, -1.8334312864604134, 41.43045545782507)\n",
      "Missing Sentinel-2 scenes for AOI (-1.9472346869945791, 41.37056606450764, -1.8334312864604134, 41.43045545782507)\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "Error stacking Sentinel-2 data: 'bands'. Skipping AOI 0\n",
      "\n",
      "Processing AOI at index 1\n",
      "Found 12 chips for AOI (-0.7193912237858445, 38.090586335295846, -0.5960662338389593, 38.187457290635244)\n",
      "Skipping chip ID 1_0 for missing values\n",
      "Skipping chip ID 1_0 for missing values\n",
      "Skipping chip ID 1_0 for missing values\n",
      "Skipping chip ID 1_0 for missing values\n",
      "Processing chip ID 1_1 for event date 2021-01-15T10:54:11.024000000\n",
      "Skipping chip ID 1_1_0 for chip 1_1 date: 20210115 — file already exists\n",
      "Skipping chip ID 1_1 for missing values\n",
      "Skipping chip ID 1_1 for missing values\n",
      "Skipping chip ID 1_1 for missing values\n",
      "Skipping chip ID 1_2 for missing values\n",
      "Skipping chip ID 1_2 for missing values\n",
      "Skipping chip ID 1_2 for missing values\n",
      "Skipping chip ID 1_2 for missing values\n",
      "Skipping chip ID 1_3 for missing values\n",
      "Skipping chip ID 1_3 for missing values\n",
      "Skipping chip ID 1_3 for missing values\n",
      "Skipping chip ID 1_3 for missing values\n",
      "Skipping chip ID 1_4 for missing values\n",
      "Skipping chip ID 1_4 for missing values\n",
      "Skipping chip ID 1_4 for missing values\n",
      "Skipping chip ID 1_4 for missing values\n",
      "Processing chip ID 1_5 for event date 2021-01-15T10:54:11.024000000\n",
      "Skipping chip ID 1_5_0 for chip 1_5 date: 20210115 — file already exists\n",
      "Skipping chip ID 1_5 for missing values\n",
      "Skipping chip ID 1_5 for missing values\n",
      "Skipping chip ID 1_5 for missing values\n"
     ]
    }
   ],
   "source": [
    "for index, aoi in summary_gdf.iterrows():\n",
    "    print(f\"\\nProcessing AOI at index {index}\")\n",
    "    # if index <=36:\n",
    "    #     continue\n",
    "\n",
    "    aoi_bounds = aoi['geometry'].bounds\n",
    "    s2_items = pystac.item_collection.ItemCollection([])\n",
    "    event_date_ranges, control_date_ranges = get_date_ranges(aoi, n_control_years=7)\n",
    "    # print(event_date_ranges)\n",
    "    for date_range in event_date_ranges:        \n",
    "        s2_items_season = search_s2_scenes(aoi, date_range, catalog, config)\n",
    "        s2_items += s2_items_season\n",
    "\n",
    "    if len(s2_items)<2 or \"bands\" not in s2_items[0].assets:\n",
    "        print(f\"Invalid or Missing Sentinel-2 scenes for AOI {aoi_bounds}\")\n",
    "        continue\n",
    "    try:\n",
    "        epsg = s2_items[0].properties[\"proj:epsg\"]\n",
    "    except:\n",
    "        epsg = int(s2_items[0].properties[\"proj:code\"].split(\":\")[-1])\n",
    "    clipping_geom = aoi[\"geometry\"]\n",
    "\n",
    "    try:\n",
    "        s2_stack = stackstac.stack(\n",
    "            s2_items,\n",
    "            assets=config[\"sentinel_2\"][\"bands\"],\n",
    "            epsg=epsg,\n",
    "            resolution=config[\"sentinel_2\"][\"resolution\"],\n",
    "            fill_value=np.nan,\n",
    "            bounds_latlon = clipping_geom.bounds\n",
    "        )\n",
    "        s2_stack = mask_cloudy_pixels(s2_stack)\n",
    "        s2_stack = s2_stack.drop_sel(band=\"SCL\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Sentinel-2 data: {e}. Skipping AOI {index}\")\n",
    "        continue\n",
    "    burn_mask = rasterize_aoi(aoi, s2_stack)\n",
    "\n",
    "    chips = crop_burn_windows(\n",
    "        s2_stack,\n",
    "        burn_mask,\n",
    "        chip_size=config['chips']['chip_size'],\n",
    "        pct_threshold=0.30,\n",
    "        )\n",
    "    \n",
    "    print(f\"Found {len(chips)} chips for AOI {aoi_bounds}\")\n",
    "    \n",
    "    for chip_id, s2_chip in enumerate(chips):\n",
    "        event_status, metadata_df = generate_fire_chips(s2_chip, aoi, config, \"event\", epsg, f\"{index}_{chip_id}\", metadata_df)\n",
    "    if event_status:\n",
    "        for control_date_range in control_date_ranges:\n",
    "            for date_range in control_date_range: \n",
    "                s2_items = search_s2_scenes(aoi, date_range, catalog, config)\n",
    "            \n",
    "                if len(s2_items)<1:\n",
    "                    print(f\"Missing Sentinel-2 scenes for AOI {aoi_bounds}\")\n",
    "                    continue\n",
    "                try:\n",
    "                    epsg = s2_items[0].properties[\"proj:epsg\"]\n",
    "                except:\n",
    "                    epsg = int(s2_items[0].properties[\"proj:code\"].split(\":\")[-1])\n",
    "                try:\n",
    "                    for it in s2_items:\n",
    "                        planetary_computer.sign_inplace(it)\n",
    "                    _open_once(s2_items[0].assets[\"bands\"][0].href)\n",
    "                    s2_stack = stackstac.stack(\n",
    "                        s2_items,\n",
    "                        assets=config[\"sentinel_2\"][\"bands\"],\n",
    "                        epsg=epsg,\n",
    "                        resolution=config[\"sentinel_2\"][\"resolution\"],\n",
    "                        fill_value=np.nan,\n",
    "                        bounds_latlon = clipping_geom.bounds\n",
    "                    )\n",
    "                    s2_stack = mask_cloudy_pixels(s2_stack)\n",
    "                    s2_stack = s2_stack.drop_sel(band=\"SCL\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error stacking Sentinel-2 data: {e}. Skipping AOI {index}\")\n",
    "                    continue\n",
    "                chips = crop_burn_windows(\n",
    "                    s2_stack,\n",
    "                    burn_mask,\n",
    "                    chip_size=config['chips']['chip_size'],\n",
    "                    pct_threshold=0.30,\n",
    "                    )\n",
    "                for chip_id, s2_chip in enumerate(chips):\n",
    "                    control_status, metadata_df = generate_fire_chips(s2_chip, aoi, config, \"control\", epsg, f\"{index}_{chip_id}\", metadata_df)\n",
    "            metadata_df.to_csv('data/metadata_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b14b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>date</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>type</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>epsg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_4</td>\n",
       "      <td>20220528</td>\n",
       "      <td>0_4000_e_20220528</td>\n",
       "      <td>event</td>\n",
       "      <td>591320.0</td>\n",
       "      <td>4581610.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_0</td>\n",
       "      <td>20220528</td>\n",
       "      <td>0_0000_e_20220528</td>\n",
       "      <td>event</td>\n",
       "      <td>595800.0</td>\n",
       "      <td>4586090.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39_9</td>\n",
       "      <td>20230316</td>\n",
       "      <td>39_900_e_20230316</td>\n",
       "      <td>event</td>\n",
       "      <td>716180.0</td>\n",
       "      <td>4214570.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39_8</td>\n",
       "      <td>20230316</td>\n",
       "      <td>39_800_e_20230316</td>\n",
       "      <td>event</td>\n",
       "      <td>713940.0</td>\n",
       "      <td>4214570.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39_7</td>\n",
       "      <td>20230316</td>\n",
       "      <td>39_700_e_20230316</td>\n",
       "      <td>event</td>\n",
       "      <td>711700.0</td>\n",
       "      <td>4214570.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>0_6</td>\n",
       "      <td>20220222</td>\n",
       "      <td>0_6000_e_20220222</td>\n",
       "      <td>event</td>\n",
       "      <td>595800.0</td>\n",
       "      <td>4581610.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0_5</td>\n",
       "      <td>20220222</td>\n",
       "      <td>0_5000_e_20220222</td>\n",
       "      <td>event</td>\n",
       "      <td>593560.0</td>\n",
       "      <td>4581610.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>0_3</td>\n",
       "      <td>20220222</td>\n",
       "      <td>0_3000_e_20220222</td>\n",
       "      <td>event</td>\n",
       "      <td>595800.0</td>\n",
       "      <td>4583850.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>0_2</td>\n",
       "      <td>20220222</td>\n",
       "      <td>0_2000_e_20220222</td>\n",
       "      <td>event</td>\n",
       "      <td>593560.0</td>\n",
       "      <td>4583850.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0_1</td>\n",
       "      <td>20220222</td>\n",
       "      <td>0_1000_e_20220222</td>\n",
       "      <td>event</td>\n",
       "      <td>591320.0</td>\n",
       "      <td>4583850.0</td>\n",
       "      <td>32630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>516 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    chip_id      date          sample_id   type  x_center   y_center   epsg\n",
       "0       0_4  20220528  0_4000_e_20220528  event  591320.0  4581610.0  32630\n",
       "1       0_0  20220528  0_0000_e_20220528  event  595800.0  4586090.0  32630\n",
       "2      39_9  20230316  39_900_e_20230316  event  716180.0  4214570.0  32630\n",
       "3      39_8  20230316  39_800_e_20230316  event  713940.0  4214570.0  32630\n",
       "4      39_7  20230316  39_700_e_20230316  event  711700.0  4214570.0  32630\n",
       "..      ...       ...                ...    ...       ...        ...    ...\n",
       "511     0_6  20220222  0_6000_e_20220222  event  595800.0  4581610.0  32630\n",
       "512     0_5  20220222  0_5000_e_20220222  event  593560.0  4581610.0  32630\n",
       "513     0_3  20220222  0_3000_e_20220222  event  595800.0  4583850.0  32630\n",
       "514     0_2  20220222  0_2000_e_20220222  event  593560.0  4583850.0  32630\n",
       "515     0_1  20220222  0_1000_e_20220222  event  591320.0  4583850.0  32630\n",
       "\n",
       "[516 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfm_bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
