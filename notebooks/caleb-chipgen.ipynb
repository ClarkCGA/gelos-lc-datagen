{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57bdd66-2d22-4c7c-ab69-6e47b8ef9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all directories and files in the current working directory\n",
    "# for root, dirs, files in os.walk('.'):\n",
    "#     print(\"Root directory:\", root)\n",
    "#     print(\"Subdirectories:\", dirs)\n",
    "#     print(\"Files:\", files)\n",
    "#     break  # Stop after the first level to avoid printing too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a464d3-d16d-4b48-88f9-8dc05b7f23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")    #commment after first run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cf645f2-1b01-4a94-a9bf-7cf315544d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import dask.distributed\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import stackstac \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from src.utils import gen_chips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1e56f6-4400-465e-a1d5-285d045430a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benchuser/code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca29bef3-a40d-4f99-84f6-1056da9176f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentinel_2': {'collection': 'sentinel-2-l2a', 'time_ranges': ['2023-01-01/2023-03-31', '2023-04-01/2023-06-30', '2023-07-01/2023-09-30', '2023-10-01/2023-12-31'], 'cloud_cover': 1, 'bands': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'], 'resolution': 10}, 'land_cover': {'collection': 'io-lulc-annual-v02', 'year': '2023-01-01/2023-12-31'}, 'chips': {'sample_size': 256, 'chip_size': 128}, 'output': {'directory': 'notebooks/test_output_dump', 'naming_convention': 's2_{season}_{index:05}.tif'}, 'metadata': {'file': 'metadata.csv'}}\n"
     ]
    }
   ],
   "source": [
    "#config setup\n",
    "import yaml\n",
    "with open(\"notebooks/config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(config)  # Check the structure of the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "057c1775-4132-4dfd-bf7d-38b449fc6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 settings\n",
    "s2_collection = config[\"sentinel_2\"][\"collection\"]\n",
    "s2_date_ranges = config[\"sentinel_2\"][\"time_ranges\"]\n",
    "s2_bands = config[\"sentinel_2\"][\"bands\"]\n",
    "s2_resolution = config[\"sentinel_2\"][\"resolution\"]\n",
    "cloud_cover_threshold = config[\"sentinel_2\"][\"cloud_cover\"]  # Max allowed cloud cover\n",
    "\n",
    "# Land Cover settings\n",
    "lc_collection = config[\"land_cover\"][\"collection\"]\n",
    "lc_year = config[\"land_cover\"][\"year\"]  # Year of LC dataset\n",
    "\n",
    "# Chip settings\n",
    "sample_size = config[\"chips\"][\"sample_size\"]  # Grid size for homogeneity check\n",
    "chip_size = config[\"chips\"][\"chip_size\"]  # Output chip size\n",
    "\n",
    "# Output settings\n",
    "output_dir = config[\"output\"][\"directory\"]\n",
    "chip_naming_convention = config[\"output\"][\"naming_convention\"]\n",
    "\n",
    "# Metadata settings\n",
    "metadata_file = config[\"metadata\"][\"file\"]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define seasons for indexing\n",
    "seasons = [\"JFM\", \"AMJ\", \"JAS\", \"OND\"]\n",
    "\n",
    "aoi_gdf = gpd.read_file(\"data/urbans.geojson\") # or \"data/aois.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6115b35c-21fe-4f5e-9697-169bf0c0a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-299532' coro=<Client._gather.<locals>.wait() done, defined at /opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/client.py:2385> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/client.py\", line 2394, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n",
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-299533' coro=<Client._gather.<locals>.wait() done, defined at /opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/client.py:2385> exception=AllExit()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/client.py\", line 2394, in wait\n",
      "    raise AllExit()\n",
      "distributed.client.AllExit\n"
     ]
    }
   ],
   "source": [
    "#dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "631c8350-dacd-4196-a5e6-b0378da78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_s2_scenes(aoi, date_range):\n",
    "    print(f\"Searching for Sentinel-2 scenes for AOI at {aoi.geometry.bounds} within {date_range}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    s2_search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        bbox=aoi.geometry.bounds,\n",
    "        datetime=date_range,\n",
    "        query=[\"eo:cloud_cover<1\"],\n",
    "        sortby=[\"+properties.eo:cloud_cover\"],\n",
    "        max_items=1,\n",
    "    )\n",
    "    items = s2_search.item_collection()\n",
    "    print(f\"Found {len(items)} Sentinel-2 scenes\")\n",
    "    return items\n",
    "\n",
    "def search_lc_scene(bbox, lc_date_range):\n",
    "    print(f\"Searching for Land Cover scenes within {lc_date_range} for bbox {bbox}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    lc_search = catalog.search(\n",
    "        collections=[\"io-lulc-annual-v02\"],\n",
    "        bbox=bbox,\n",
    "        datetime=lc_date_range,\n",
    "    )\n",
    "    items = lc_search.item_collection()\n",
    "    print(f\"Found {len(items)} Land Cover scenes\")\n",
    "    return items\n",
    "\n",
    "def stack_s2_data(s2_items, s2_assets):\n",
    "    if not s2_items:\n",
    "        print(\"No Sentinel-2 items found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Stacking Sentinel-2 images...\")\n",
    "        stacked_data = stackstac.stack(\n",
    "            s2_items,\n",
    "            assets=s2_assets,\n",
    "            epsg=s2_items[0].properties[\"proj:epsg\"],\n",
    "            resolution=10,\n",
    "            bounds_latlon=s2_items[0].bbox,\n",
    "        ).median(\"time\", skipna=True).squeeze()\n",
    "\n",
    "        print(\"Stacked S2 data shape:\", stacked_data.shape)\n",
    "\n",
    "        if stacked_data is None or not isinstance(stacked_data, xr.DataArray):\n",
    "            print(\"Error: Stacking Sentinel-2 data resulted in an invalid output.\")\n",
    "            return None\n",
    "\n",
    "        return stacked_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Sentinel-2 data: {e}\")\n",
    "        return None\n",
    "\n",
    "def stack_lc_data(lc_items, s2_epsg):\n",
    "    if len(lc_items) == 0:\n",
    "        print(\"No Land Cover data found.\")\n",
    "        return None\n",
    "    try:\n",
    "        print(\"Stacking Land Cover images...\")\n",
    "        stacked_data = stackstac.stack(\n",
    "            lc_items,\n",
    "            dtype=np.ubyte,\n",
    "            fill_value=255,\n",
    "            sortby_date=False,\n",
    "            epsg=s2_epsg,\n",
    "            resolution=10,\n",
    "            bounds_latlon=lc_items[0].bbox,\n",
    "        ).squeeze()\n",
    "        print(\"Stacked LC data shape:\", stacked_data.shape)\n",
    "        return stacked_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Land Cover data: {e}\")\n",
    "        return None\n",
    "\n",
    "def unique_class(window, axis=None, **kwargs):\n",
    "    return np.all(window == window[:, :1, :, :1], axis=(1, 3))\n",
    "\n",
    "\n",
    "def is_homogeneous(lc_stack):\n",
    "    print(\"Checking for homogeneous LC regions...\")\n",
    "\n",
    "    # Ensure dimensions are large enough to process\n",
    "    height, width = lc_stack.shape[-2], lc_stack.shape[-1]\n",
    "\n",
    "    if height < sample_size or width < sample_size:\n",
    "        print(f\"AOI too small ({height}, {width}) for processing. Skipping this AOI.\")\n",
    "        return None  # Prevent infinite looping\n",
    "\n",
    "    # Compute cropping dimensions\n",
    "    new_height = max((height // sample_size) * sample_size, sample_size)\n",
    "    new_width = max((width // sample_size) * sample_size, sample_size)\n",
    "\n",
    "    print(\"old height and width:\", height, width, \"new height and width:\", new_height, new_width)\n",
    "\n",
    "    if new_height != height or new_width != width:\n",
    "        print(f\"Cropping LC stack to ({new_height}, {new_width}) to match sample_size.\")\n",
    "        lc_stack = lc_stack.isel(x=slice(0, new_height), y=slice(0, new_width))\n",
    "\n",
    "    # Drop mismatched coordinate variables\n",
    "    lc_stack = lc_stack.drop_vars([var for var in lc_stack.coords if \"x\" in var or \"y\" in var], errors=\"ignore\")\n",
    "\n",
    "    # Ensure LC stack is still valid before coarsening\n",
    "    if lc_stack.shape[-2] < sample_size or lc_stack.shape[-1] < sample_size:\n",
    "        print(\"Error: LC stack is too small after cropping. Skipping this AOI.\")\n",
    "        return None\n",
    "\n",
    "    # Apply coarsening\n",
    "    return lc_stack.coarsen(x=sample_size, y=sample_size, boundary=\"trim\").reduce(unique_class)\n",
    "    \n",
    "def has_missing_values(array):\n",
    "    print(\"Checking for missing values...\")\n",
    "    return array.isnull().any().compute()\n",
    "\n",
    "def process_chips(aoi, s2_stack, lc_stack, output_dir, global_index):\n",
    "    print(f\"Processing chips for AOI at {aoi.geometry.bounds}\")\n",
    "\n",
    "    #  Compute homogeneity only ONCE before looping\n",
    "    lc_uniqueness = is_homogeneous(lc_stack)\n",
    "    if lc_uniqueness is None:\n",
    "        print(f\"Skipping AOI at {aoi.geometry.bounds}: No valid LC regions.\")\n",
    "        return global_index  # Skip processing if LC data is invalid\n",
    "\n",
    "    for i in range(0, lc_stack.shape[1] - chip_size, sample_size):\n",
    "        for j in range(0, lc_stack.shape[2] - chip_size, sample_size):\n",
    "            if lc_uniqueness.isel(x=i//sample_size, y=j//sample_size) == False:\n",
    "                print(f\"Skipping chip at ({i}, {j}): LC region not homogeneous.\")\n",
    "                continue  \n",
    "           \n",
    "            s2_chip = s2_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "            lc_chip = lc_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "\n",
    "            if has_missing_values(s2_chip):\n",
    "                print(f\"Skipping chip at ({i}, {j}): Contains missing values.\")\n",
    "                continue  \n",
    "\n",
    "            chip_name = f\"s2_{date_range[:3]}_{global_index:05}.tif\"\n",
    "            lc_chip.rio.to_raster(os.path.join(output_dir, f\"lc_{chip_name}\"))\n",
    "            s2_chip.rio.to_raster(os.path.join(output_dir, chip_name))\n",
    "            print(f\"Saved chip {chip_name}\")\n",
    "            global_index += 1\n",
    "\n",
    "    return global_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39eb84a0-71bc-4f8e-bc98-1e92dd6e900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AOI 0 at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983)\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-01-01/2023-03-31\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-04-01/2023-06-30\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-07-01/2023-09-30\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-10-01/2023-12-31\n",
      "Found 1 Sentinel-2 scenes\n",
      "Stacking Sentinel-2 images...\n",
      "Stacked S2 data shape: (2, 11272, 11273)\n",
      "Error: s2_stack is missing CRS information. Assigning manually.\n",
      "Sentinel-2 CRS: EPSG:32636\n",
      "Searching for Land Cover scenes within 2023-01-01/2023-12-31 for bbox [30.91139663, 29.72604903, 32.06723105, 30.72965025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Land Cover scenes\n",
      "Stacking Land Cover images...\n",
      "Stacked LC data shape: (2, 89417, 65754)\n",
      "Processing chips for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983)\n",
      "Checking for homogeneous LC regions...\n",
      "old height and width: 89417 65754 new height and width: 89344 65536\n",
      "Cropping LC stack to (89344, 65536) to match sample_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Process chips\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m global_index, chip_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_chips\u001b[49m\u001b[43m(\u001b[49m\u001b[43maoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlc_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 132\u001b[0m, in \u001b[0;36mprocess_chips\u001b[0;34m(aoi, s2_stack, lc_stack, output_dir, global_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, lc_stack\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m chip_size, sample_size):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, lc_stack\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m chip_size, sample_size):\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m lc_uniqueness\u001b[38;5;241m.\u001b[39misel(x\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39msample_size, y\u001b[38;5;241m=\u001b[39mj\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39msample_size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping chip at (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): LC region not homogeneous.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    134\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m  \n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/xarray/core/common.py:154\u001b[0m, in \u001b[0;36mAbstractArray.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/xarray/core/dataarray.py:814\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;124;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/xarray/core/variable.py:566\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    565\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/xarray/core/variable.py:363\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/dask/array/core.py:1696\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1696\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n\u001b[1;32m   1698\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/dask/base.py:372\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/dask/base.py:660\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 660\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Main processing loop\n",
    "chip_dict = {}\n",
    "global_index = 0\n",
    "for index, aoi in aoi_gdf.iterrows():\n",
    "    print(f\"Processing AOI {index} at {aoi.geometry.bounds}\")\n",
    "\n",
    "    # Search for Sentinel-2 images\n",
    "    s2_items = [search_s2_scenes(aoi, date_range) for date_range in s2_date_ranges]\n",
    "    \n",
    "    # Ensure all quarters have data\n",
    "    if any(len(items) == 0 for items in s2_items):\n",
    "        print(f\"Skipping AOI {index}: Missing Sentinel-2 data for at least one quarter.\")\n",
    "        continue\n",
    "\n",
    "    # Extract first valid Sentinel-2 item\n",
    "    s2_first_item = next(iter(s2_items[0]), None)  # Ensure this exists\n",
    "    if s2_first_item is None:\n",
    "        print(\"Skipping AOI: No valid Sentinel-2 item found.\")\n",
    "        continue\n",
    "\n",
    "    # Stack Sentinel-2 images\n",
    "    s2_stack = stack_s2_data(\n",
    "        [next(iter(item), None) for item in s2_items if item], s2_bands\n",
    "    )\n",
    "\n",
    "    # ✅ Ensure s2_stack is valid\n",
    "    if s2_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid Sentinel-2 data found.\")\n",
    "        continue\n",
    "\n",
    "    if not hasattr(s2_stack, \"rio\") or s2_stack.rio.crs is None:\n",
    "        print(f\"Error: s2_stack is missing CRS information. Assigning manually.\")\n",
    "        s2_epsg = s2_first_item.properties[\"proj:epsg\"]\n",
    "        s2_stack.rio.write_crs(f\"epsg:{s2_epsg}\", inplace=True)\n",
    "\n",
    "    print(f\"Sentinel-2 CRS: {s2_stack.rio.crs}\")\n",
    "\n",
    "    # Search for Land Cover images using `s2_first_item.bbox`\n",
    "    lc_items = search_lc_scene(s2_first_item.bbox, lc_year)\n",
    "    \n",
    "    if len(lc_items) == 0:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # ✅ Now safe to extract EPSG and stack LC data\n",
    "    lc_stack = stack_lc_data(lc_items, s2_stack.rio.crs.to_epsg())\n",
    "\n",
    "    if lc_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # Process chips\n",
    "    global_index, chip_dict = process_chips(aoi, s2_stack, lc_stack, output_dir, global_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911b0d4a-7f3a-435d-a784-f20a43a703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional plotting code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the first stored location\n",
    "if chip_dict:\n",
    "    first_location = next(iter(chip_dict.keys()))  # Get the first (i, j) key\n",
    "    first_four_chips = chip_dict[first_location]  # Get the corresponding chips\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 6))  # 2 rows, 4 columns\n",
    "\n",
    "    for idx, (s2_chip, lc_chip) in enumerate(first_four_chips):\n",
    "        # Sentinel-2 RGB Visualization (assuming B4, B3, B2 are RGB)\n",
    "        rgb_image = s2_chip.sel(band=[3, 2, 1]).transpose(\"y\", \"x\", \"band\")  # Adjust band indices if needed\n",
    "        axes[0, idx].imshow(rgb_image.compute().clip(0, 3000) / 3000)  # Normalize for better visualization\n",
    "        axes[0, idx].set_title(f\"Sentinel-2 Chip {idx+1}\")\n",
    "        axes[0, idx].axis(\"off\")\n",
    "\n",
    "        # Land Cover Visualization\n",
    "        axes[1, idx].imshow(lc_chip.compute(), cmap=\"tab10\")  # Color map for categorical data\n",
    "        axes[1, idx].set_title(f\"Land Cover Chip {idx+1}\")\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dbbc00-0965-493e-9567-1f0e556f6b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(chip_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
