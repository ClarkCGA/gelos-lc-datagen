{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57bdd66-2d22-4c7c-ab69-6e47b8ef9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all directories and files in the current working directory\n",
    "# for root, dirs, files in os.walk('.'):\n",
    "#     print(\"Root directory:\", root)\n",
    "#     print(\"Subdirectories:\", dirs)\n",
    "#     print(\"Files:\", files)\n",
    "#     break  # Stop after the first level to avoid printing too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1a464d3-d16d-4b48-88f9-8dc05b7f23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")    #commment after first run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cf645f2-1b01-4a94-a9bf-7cf315544d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import dask.distributed\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import stackstac \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from src.utils import gen_chips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad1e56f6-4400-465e-a1d5-285d045430a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benchuser/code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "#should be /home/benchuser/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fbeaa45-046c-4e4a-9c41-8c93278fb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/benchuser/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca29bef3-a40d-4f99-84f6-1056da9176f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentinel_2': {'collection': 'sentinel-2-l2a', 'time_ranges': ['2023-01-01/2023-03-31', '2023-04-01/2023-06-30', '2023-07-01/2023-09-30', '2023-10-01/2023-12-31'], 'cloud_cover': 1, 'bands': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'], 'resolution': 10}, 'land_cover': {'collection': 'io-lulc-annual-v02', 'year': '2023-01-01/2023-12-31'}, 'chips': {'sample_size': 256, 'chip_size': 128}, 'output': {'directory': 'notebooks/test_output_dump', 'naming_convention': 's2_{season}_{index:05}.tif'}, 'metadata': {'file': 'metadata.csv'}}\n"
     ]
    }
   ],
   "source": [
    "#config setup\n",
    "import yaml\n",
    "with open(\"notebooks/config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(config)  # Check the structure of the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "057c1775-4132-4dfd-bf7d-38b449fc6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 settings\n",
    "s2_collection = config[\"sentinel_2\"][\"collection\"]\n",
    "s2_date_ranges = config[\"sentinel_2\"][\"time_ranges\"]\n",
    "s2_bands = config[\"sentinel_2\"][\"bands\"]\n",
    "s2_resolution = config[\"sentinel_2\"][\"resolution\"]\n",
    "cloud_cover_threshold = config[\"sentinel_2\"][\"cloud_cover\"]  # Max allowed cloud cover\n",
    "\n",
    "# Land Cover settings\n",
    "lc_collection = config[\"land_cover\"][\"collection\"]\n",
    "lc_year = config[\"land_cover\"][\"year\"]  # Year of LC dataset\n",
    "\n",
    "# Chip settings\n",
    "sample_size = config[\"chips\"][\"sample_size\"]  # Grid size for homogeneity check\n",
    "chip_size = config[\"chips\"][\"chip_size\"]  # Output chip size\n",
    "\n",
    "# Output settings\n",
    "output_dir = config[\"output\"][\"directory\"]\n",
    "chip_naming_convention = config[\"output\"][\"naming_convention\"]\n",
    "\n",
    "# Metadata settings\n",
    "metadata_file = config[\"metadata\"][\"file\"]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define seasons for indexing\n",
    "seasons = [\"JFM\", \"AMJ\", \"JAS\", \"OND\"]\n",
    "\n",
    "aoi_gdf = gpd.read_file(\"data/urbans.geojson\") # or \"data/aois.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "699d17f6-4269-4995-bd18-cb6aa57406db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old AOI Bounding Box: (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983)\n",
      "New AOI Bounding Box for Testing: [31.238484605190482, 30.0401298362078, 31.246111701174524, 30.043853928174013]\n"
     ]
    }
   ],
   "source": [
    "aoi_bounds = aoi.geometry.bounds  # This gives (minx, miny, maxx, maxy)\n",
    "# Extract coordinates\n",
    "minx, miny, maxx, maxy = aoi_bounds\n",
    "\n",
    "# Reduce size by 50% (adjustable)\n",
    "shrink_factor = 0.1  \n",
    "center_x = (minx + maxx) / 2\n",
    "center_y = (miny + maxy) / 2\n",
    "\n",
    "# Compute new bounds\n",
    "new_minx = center_x - (maxx - minx) * shrink_factor / 2\n",
    "new_maxx = center_x + (maxx - minx) * shrink_factor / 2\n",
    "new_miny = center_y - (maxy - miny) * shrink_factor / 2\n",
    "new_maxy = center_y + (maxy - miny) * shrink_factor / 2\n",
    "\n",
    "# Create new AOI dictionary (to pass into bbox-based queries)\n",
    "aoi_test_bbox = [new_minx, new_miny, new_maxx, new_maxy]\n",
    "\n",
    "print(\"Old AOI Bounding Box:\", aoi_bounds)\n",
    "\n",
    "print(\"New AOI Bounding Box for Testing:\", aoi_test_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6115b35c-21fe-4f5e-9697-169bf0c0a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37567 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:37567/status\n"
     ]
    }
   ],
   "source": [
    "#dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "631c8350-dacd-4196-a5e6-b0378da78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_s2_scenes(aoi, date_range):\n",
    "    print(f\"Searching for Sentinel-2 scenes for AOI at {aoi.geometry.bounds} within {date_range}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    s2_search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        #bbox=aoi.geometry.bounds, #UNCOMMENT FOR FULL AOI\n",
    "        bbox = aoi_test_bbox,\n",
    "        datetime=date_range,\n",
    "        query=[\"eo:cloud_cover<1\"],\n",
    "        sortby=[\"+properties.eo:cloud_cover\"],\n",
    "        max_items=1,\n",
    "    )\n",
    "    items = s2_search.item_collection()\n",
    "    print(f\"Found {len(items)} Sentinel-2 scenes\")\n",
    "    return items\n",
    "\n",
    "def search_lc_scene(bbox, lc_date_range):\n",
    "    print(f\"Searching for Land Cover scenes within {lc_date_range} for bbox {bbox}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    lc_search = catalog.search(\n",
    "        collections=[\"io-lulc-annual-v02\"],\n",
    "        bbox=bbox,\n",
    "        datetime=lc_date_range,\n",
    "    )\n",
    "    items = lc_search.item_collection()\n",
    "    print(f\"Found {len(items)} Land Cover scenes\")\n",
    "    return items\n",
    "\n",
    "def stack_s2_data(s2_items, s2_assets):\n",
    "    if not s2_items:\n",
    "        print(\"No Sentinel-2 items found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Stacking Sentinel-2 images...\")\n",
    "        stacked_data = stackstac.stack(\n",
    "            s2_items,\n",
    "            assets=s2_assets,\n",
    "            epsg=s2_items[0].properties[\"proj:epsg\"],\n",
    "            resolution=10,\n",
    "            bounds_latlon=s2_items[0].bbox,\n",
    "        ).median(\"time\", skipna=True).squeeze()\n",
    "\n",
    "        print(\"Stacked S2 data shape:\", stacked_data.shape)\n",
    "\n",
    "        if stacked_data is None or not isinstance(stacked_data, xr.DataArray):\n",
    "            print(\"Error: Stacking Sentinel-2 data resulted in an invalid output.\")\n",
    "            return None\n",
    "\n",
    "        return stacked_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Sentinel-2 data: {e}\")\n",
    "        return None\n",
    "\n",
    "def stack_lc_data(lc_items, s2_epsg):\n",
    "    if len(lc_items) == 0:\n",
    "        print(\"No Land Cover data found.\")\n",
    "        return None\n",
    "    try:\n",
    "        print(\"Stacking Land Cover images...\")\n",
    "        stacked_data = stackstac.stack(\n",
    "            lc_items,\n",
    "            dtype=np.ubyte,\n",
    "            fill_value=255,\n",
    "            sortby_date=False,\n",
    "            epsg=s2_epsg,\n",
    "            resolution=10,\n",
    "            bounds_latlon=lc_items[0].bbox,\n",
    "        ).squeeze()\n",
    "        print(\"Stacked LC data shape:\", stacked_data.shape)\n",
    "        return stacked_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Land Cover data: {e}\")\n",
    "        return None\n",
    "\n",
    "def unique_class(window, axis=None, **kwargs):\n",
    "    return np.all(window == window[:, :1, :, :1], axis=(1, 3))\n",
    "\n",
    "\n",
    "def is_homogeneous(lc_stack):\n",
    "    print(\"Checking for homogeneous LC regions...\")\n",
    "\n",
    "    # Ensure dimensions are exactly divisible by `sample_size`\n",
    "    height, width = lc_stack.shape[-2], lc_stack.shape[-1]\n",
    "    new_height = (height // sample_size) * sample_size\n",
    "    new_width = (width // sample_size) * sample_size\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        print(\"Error: LC stack is empty after cropping. Skipping this AOI.\")\n",
    "        return None  \n",
    "\n",
    "    if new_height != height or new_width != width:\n",
    "        print(f\"Cropping LC stack to ({new_height}, {new_width}) to match sample_size.\")\n",
    "        lc_stack = lc_stack.isel(x=slice(0, new_height), y=slice(0, new_width))\n",
    "\n",
    "    # Drop mismatched coordinate variables\n",
    "    lc_stack = lc_stack.drop_vars([var for var in lc_stack.coords if \"x\" in var or \"y\" in var], errors=\"ignore\")\n",
    "\n",
    "    # Ensure rechunking aligns with the coarsening window\n",
    "    lc_stack = lc_stack.chunk({\"x\": sample_size, \"y\": sample_size})\n",
    "\n",
    "    # Check if lc_stack is empty after all modifications\n",
    "    if lc_stack.size == 0:\n",
    "        print(\"Error: LC stack is empty after cropping. Skipping this AOI.\")\n",
    "        return None  \n",
    "\n",
    "    # Apply coarsening and reduce with unique_class function\n",
    "    lc_uniqueness = lc_stack.coarsen(x=sample_size, y=sample_size, boundary=\"trim\").reduce(unique_class)\n",
    "\n",
    "    # Debugging: Print dtype of lc_uniqueness\n",
    "    print(f\"LC uniqueness dtype: {lc_uniqueness.dtype}, shape: {lc_uniqueness.shape}, type: {type(lc_uniqueness)}\")\n",
    "\n",
    "    return lc_uniqueness\n",
    "\n",
    "    \n",
    "def has_missing_values(array):\n",
    "    print(\"Checking for missing values...\")\n",
    "    return array.isnull().any().compute()\n",
    "\n",
    "def process_chips(aoi, s2_stack, lc_stack, output_dir, global_index):\n",
    "    print(f\"Processing chips for AOI at {aoi.geometry.bounds}\")\n",
    "\n",
    "    # Compute lc_uniqueness before looping\n",
    "    lc_uniqueness = is_homogeneous(lc_stack).compute()\n",
    "    print(f\"LC uniqueness dtype after compute: {lc_uniqueness.dtype}, shape: {lc_uniqueness.shape}\")\n",
    "\n",
    "    for i in range(0, lc_stack.shape[1] - chip_size, sample_size):\n",
    "        for j in range(0, lc_stack.shape[2] - chip_size, sample_size):\n",
    "            \n",
    "            # Ensure indices stay within valid bounds\n",
    "            x_index = min(i // sample_size, lc_uniqueness.sizes[\"x\"] - 1)\n",
    "            y_index = min(j // sample_size, lc_uniqueness.sizes[\"y\"] - 1)\n",
    "\n",
    "            # Extract boolean value safely\n",
    "            value = lc_uniqueness.isel(x=x_index, y=y_index).values\n",
    "            if isinstance(value, np.ndarray):  \n",
    "                value = value.item()  # Ensure it's a single boolean value\n",
    "\n",
    "            if not value:\n",
    "                print(f\"Skipping chip at ({i}, {j}): LC region not homogeneous.\")\n",
    "                continue  \n",
    "\n",
    "            s2_chip = s2_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "            lc_chip = lc_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "\n",
    "            if has_missing_values(s2_chip):\n",
    "                continue  \n",
    "\n",
    "            chip_name = f\"s2_{date_range[:3]}_{global_index:05}.tif\"\n",
    "            lc_chip.rio.to_raster(os.path.join(output_dir, f\"lc_{chip_name}\"))\n",
    "            s2_chip.rio.to_raster(os.path.join(output_dir, chip_name))\n",
    "\n",
    "            print(f\"Saved chip {chip_name}\")\n",
    "            global_index += 1\n",
    "\n",
    "    return global_index\n",
    "\n",
    "def crop_to_aoi(stack, bbox):\n",
    "    \"\"\"Crop the Sentinel-2 stack to match the AOI bounding box\"\"\"\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    return stack.rio.clip_box(minx, miny, maxx, maxy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39eb84a0-71bc-4f8e-bc98-1e92dd6e900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AOI 0 at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983)\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-01-01/2023-03-31\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983) within 2023-04-01/2023-06-30\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "The request exceeded the maximum allowed time, please try again. If the issue persists, please contact planetarycomputer@microsoft.com.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing AOI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maoi\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mbounds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Search for Sentinel-2 images\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m s2_items \u001b[38;5;241m=\u001b[39m [\u001b[43msearch_s2_scenes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_range\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m date_range \u001b[38;5;129;01min\u001b[39;00m s2_date_ranges]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m items \u001b[38;5;129;01min\u001b[39;00m s2_items):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping AOI \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Missing Sentinel-2 data for at least one quarter.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[49], line 16\u001b[0m, in \u001b[0;36msearch_s2_scenes\u001b[0;34m(aoi, date_range)\u001b[0m\n\u001b[1;32m      3\u001b[0m catalog \u001b[38;5;241m=\u001b[39m pystac_client\u001b[38;5;241m.\u001b[39mClient\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://planetarycomputer.microsoft.com/api/stac/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     modifier\u001b[38;5;241m=\u001b[39mplanetary_computer\u001b[38;5;241m.\u001b[39msign_inplace,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m s2_search \u001b[38;5;241m=\u001b[39m catalog\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m      8\u001b[0m     collections\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentinel-2-l2a\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#bbox=aoi.geometry.bounds, #UNCOMMENT FOR FULL AOI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     max_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m items \u001b[38;5;241m=\u001b[39m \u001b[43ms2_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(items)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sentinel-2 scenes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m items\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/item_search.py:765\u001b[0m, in \u001b[0;36mItemSearch.item_collection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;124;03mGet the matching items as a :py:class:`pystac.ItemCollection`.\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m \u001b[38;5;124;03mReturn:\u001b[39;00m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;124;03m    ItemCollection: The item collection\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m# Bypass the cache here, so that we can pass __preserve_dict__\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# without mutating what's in the cache.\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m feature_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem_collection_as_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# already signed in item_collection_as_dict\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ItemCollection\u001b[38;5;241m.\u001b[39mfrom_dict(\n\u001b[1;32m    768\u001b[0m     feature_collection, preserve_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\n\u001b[1;32m    769\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/item_search.py:786\u001b[0m, in \u001b[0;36mItemSearch.item_collection_as_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03mGet the matching items as an item-collection-like dict.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;124;03m    Dict : A GeoJSON FeatureCollection\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    785\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 786\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpages_as_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/item_search.py:736\u001b[0m, in \u001b[0;36mItemSearch.pages_as_dicts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stac_io, StacApiIO):\n\u001b[1;32m    735\u001b[0m     num_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 736\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stac_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcall_modifier\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/stac_api_io.py:294\u001b[0m, in \u001b[0;36mStacApiIO.get_pages\u001b[0;34m(self, url, method, parameters)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_pages\u001b[39m(\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    284\u001b[0m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    285\u001b[0m     method: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    286\u001b[0m     parameters: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterator that yields dictionaries for each page at a STAC paging\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    endpoint, e.g., /collections, /search\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    Return:\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;03m        Dict[str, Any] : JSON content from a single page\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollections\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac/stac_io.py:206\u001b[0m, in \u001b[0;36mStacIO.read_json\u001b[0;34m(self, source, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, source: HREF, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    190\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read a dict from the given source.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    See :func:`StacIO.read_text <pystac.StacIO.read_text>` for usage of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m        given source.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjson_loads(txt)\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/stac_api_io.py:168\u001b[0m, in \u001b[0;36mStacApiIO.read_text\u001b[0;34m(self, source, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m href \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(source)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_url(href):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(href) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/pystac_client/stac_api_io.py:217\u001b[0m, in \u001b[0;36mStacApiIO.request\u001b[0;34m(self, href, method, headers, parameters)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError(\u001b[38;5;28mstr\u001b[39m(err))\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIError\u001b[38;5;241m.\u001b[39mfrom_response(resp)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAPIError\u001b[0m: The request exceeded the maximum allowed time, please try again. If the issue persists, please contact planetarycomputer@microsoft.com.\n\n"
     ]
    }
   ],
   "source": [
    "## Main processing loop\n",
    "chip_dict = {}\n",
    "global_index = 0\n",
    "# Main processing loop\n",
    "for index, aoi in aoi_gdf.iterrows():\n",
    "    print(f\"Processing AOI {index} at {aoi.geometry.bounds}\")\n",
    "\n",
    "    # Search for Sentinel-2 images\n",
    "    s2_items = [search_s2_scenes(aoi, date_range) for date_range in s2_date_ranges]\n",
    "    \n",
    "    if any(len(items) == 0 for items in s2_items):\n",
    "        print(f\"Skipping AOI {index}: Missing Sentinel-2 data for at least one quarter.\")\n",
    "        continue\n",
    "\n",
    "    # Stack Sentinel-2 images\n",
    "    s2_stack = stack_s2_data([item[0] for item in s2_items if item], s2_bands)\n",
    "\n",
    "    # âœ… Crop Sentinel-2 stack to AOI to reduce size\n",
    "    if s2_stack is not None:\n",
    "        print(f\"Original Sentinel-2 stack shape: {s2_stack.shape}\")\n",
    "        s2_stack = crop_to_aoi(s2_stack, aoi.geometry.bounds)  # Crop it\n",
    "        print(f\"Cropped Sentinel-2 stack shape: {s2_stack.shape}\")\n",
    "\n",
    "    # Ensure valid Sentinel-2 stack before proceeding\n",
    "    if s2_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid Sentinel-2 data found after cropping.\")\n",
    "        continue\n",
    "\n",
    "    if not hasattr(s2_stack, \"rio\") or s2_stack.rio.crs is None:\n",
    "        print(f\"Error: s2_stack is missing CRS information. Assigning manually.\")\n",
    "        s2_first_item = next(iter(s2_items), None)  # Get the first actual item\n",
    "        if s2_first_item is None:\n",
    "            print(\"Error: No valid Sentinel-2 item found.\")\n",
    "            continue\n",
    "\n",
    "        s2_epsg = s2_first_item.properties[\"proj:epsg\"]\n",
    "        s2_stack.rio.write_crs(f\"epsg:{s2_epsg}\", inplace=True)\n",
    "\n",
    "    print(f\"Sentinel-2 CRS: {s2_stack.rio.crs}\")\n",
    "\n",
    "    # Search for Land Cover images\n",
    "    lc_items = search_lc_scene(s2_items[0][0].bbox, lc_year)\n",
    "    \n",
    "    if len(lc_items) == 0:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # Stack and process LC data\n",
    "    lc_stack = stack_lc_data(lc_items, s2_stack.rio.crs.to_epsg())\n",
    "\n",
    "    if lc_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # Process chips\n",
    "    global_index, chip_dict = process_chips(aoi, s2_stack, lc_stack, output_dir, global_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b0d4a-7f3a-435d-a784-f20a43a703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional plotting code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the first stored location\n",
    "if chip_dict:\n",
    "    first_location = next(iter(chip_dict.keys()))  # Get the first (i, j) key\n",
    "    first_four_chips = chip_dict[first_location]  # Get the corresponding chips\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 6))  # 2 rows, 4 columns\n",
    "\n",
    "    for idx, (s2_chip, lc_chip) in enumerate(first_four_chips):\n",
    "        # Sentinel-2 RGB Visualization (assuming B4, B3, B2 are RGB)\n",
    "        rgb_image = s2_chip.sel(band=[3, 2, 1]).transpose(\"y\", \"x\", \"band\")  # Adjust band indices if needed\n",
    "        axes[0, idx].imshow(rgb_image.compute().clip(0, 3000) / 3000)  # Normalize for better visualization\n",
    "        axes[0, idx].set_title(f\"Sentinel-2 Chip {idx+1}\")\n",
    "        axes[0, idx].axis(\"off\")\n",
    "\n",
    "        # Land Cover Visualization\n",
    "        axes[1, idx].imshow(lc_chip.compute(), cmap=\"tab10\")  # Color map for categorical data\n",
    "        axes[1, idx].set_title(f\"Land Cover Chip {idx+1}\")\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbbc00-0965-493e-9567-1f0e556f6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chip_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb7a68-73e3-4c86-97dc-50ec125e3791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
