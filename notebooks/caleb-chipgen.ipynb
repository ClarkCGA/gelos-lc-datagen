{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57bdd66-2d22-4c7c-ab69-6e47b8ef9579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List all directories and files in the current working directory\n",
    "# for root, dirs, files in os.walk('.'):\n",
    "#     print(\"Root directory:\", root)\n",
    "#     print(\"Subdirectories:\", dirs)\n",
    "#     print(\"Files:\", files)\n",
    "#     break  # Stop after the first level to avoid printing too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a464d3-d16d-4b48-88f9-8dc05b7f23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")    #commment after first run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cf645f2-1b01-4a94-a9bf-7cf315544d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import dask.distributed\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import stackstac \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "from src.utils import gen_chips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad1e56f6-4400-465e-a1d5-285d045430a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/benchuser/code\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "#should be /home/benchuser/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9fbeaa45-046c-4e4a-9c41-8c93278fb634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"/home/benchuser/code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca29bef3-a40d-4f99-84f6-1056da9176f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentinel_2': {'collection': 'sentinel-2-l2a', 'time_ranges': ['2023-01-01/2023-03-31', '2023-04-01/2023-06-30', '2023-07-01/2023-09-30', '2023-10-01/2023-12-31'], 'cloud_cover': 1, 'bands': ['B2', 'B3', 'B4', 'B8', 'B11', 'B12'], 'resolution': 10}, 'land_cover': {'collection': 'io-lulc-annual-v02', 'year': '2023-01-01/2023-12-31'}, 'chips': {'sample_size': 256, 'chip_size': 128}, 'output': {'directory': 'notebooks/test_output_dump', 'naming_convention': 's2_{season}_{index:05}.tif'}, 'metadata': {'file': 'metadata.csv'}}\n"
     ]
    }
   ],
   "source": [
    "#config setup\n",
    "import yaml\n",
    "with open(\"notebooks/config.yml\", \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "print(config)  # Check the structure of the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "057c1775-4132-4dfd-bf7d-38b449fc6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel-2 settings\n",
    "s2_collection = config[\"sentinel_2\"][\"collection\"]\n",
    "s2_date_ranges = config[\"sentinel_2\"][\"time_ranges\"]\n",
    "s2_bands = config[\"sentinel_2\"][\"bands\"]\n",
    "s2_resolution = config[\"sentinel_2\"][\"resolution\"]\n",
    "cloud_cover_threshold = config[\"sentinel_2\"][\"cloud_cover\"]  # Max allowed cloud cover\n",
    "\n",
    "# Land Cover settings\n",
    "lc_collection = config[\"land_cover\"][\"collection\"]\n",
    "lc_year = config[\"land_cover\"][\"year\"]  # Year of LC dataset\n",
    "\n",
    "# Chip settings\n",
    "sample_size = config[\"chips\"][\"sample_size\"]  # Grid size for homogeneity check\n",
    "chip_size = config[\"chips\"][\"chip_size\"]  # Output chip size\n",
    "\n",
    "# Output settings\n",
    "output_dir = config[\"output\"][\"directory\"]\n",
    "chip_naming_convention = config[\"output\"][\"naming_convention\"]\n",
    "\n",
    "# Metadata settings\n",
    "metadata_file = config[\"metadata\"][\"file\"]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define seasons for indexing\n",
    "seasons = [\"JFM\", \"AMJ\", \"JAS\", \"OND\"]\n",
    "\n",
    "aoi_gdf = gpd.read_file(\"data/urbans.geojson\") # or \"data/aois.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "699d17f6-4269-4995-bd18-cb6aa57406db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the first AOI using .iterrows()\n",
    "# for _, aoi in aoi_gdf.iterrows():\n",
    "#     aoi_bounds = aoi.geometry.bounds\n",
    "#     break  # Exit after the first row\n",
    "# # Extract coordinates\n",
    "# minx, miny, maxx, maxy = aoi_bounds\n",
    "\n",
    "# # Reduce size by 50% (adjustable)\n",
    "# shrink_factor = 0.5  \n",
    "# center_x = (minx + maxx) / 2\n",
    "# center_y = (miny + maxy) / 2\n",
    "\n",
    "# # Compute new bounds\n",
    "# new_minx = center_x - (maxx - minx) * shrink_factor / 2\n",
    "# new_maxx = center_x + (maxx - minx) * shrink_factor / 2\n",
    "# new_miny = center_y - (maxy - miny) * shrink_factor / 2\n",
    "# new_maxy = center_y + (maxy - miny) * shrink_factor / 2\n",
    "\n",
    "# # Create new AOI dictionary (to pass into bbox-based queries)\n",
    "# aoi_test_bbox = [new_minx, new_miny, new_maxx, new_maxy]\n",
    "\n",
    "# print(\"Old AOI Bounding Box:\", aoi_bounds)\n",
    "\n",
    "# print(\"New AOI Bounding Box for Testing:\", aoi_test_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6115b35c-21fe-4f5e-9697-169bf0c0a6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40191 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:40191/status\n"
     ]
    }
   ],
   "source": [
    "#dask\n",
    "from dask.distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=8, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "631c8350-dacd-4196-a5e6-b0378da78b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_s2_scenes(aoi, date_range):\n",
    "    print(f\"Searching for Sentinel-2 scenes for AOI at {aoi_test_bbox} within {date_range}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    s2_search = catalog.search(\n",
    "        collections=[\"sentinel-2-l2a\"],\n",
    "        #bbox=aoi.geometry.bounds, #UNCOMMENT FOR FULL AOI\n",
    "        bbox = aoi_test_bbox,\n",
    "        datetime=date_range,\n",
    "        query=[\"eo:cloud_cover<1\"],\n",
    "        sortby=[\"+properties.eo:cloud_cover\"],\n",
    "        max_items=1,\n",
    "    )\n",
    "    items = s2_search.item_collection()\n",
    "    print(f\"Found {len(items)} Sentinel-2 scenes\")\n",
    "    return items\n",
    "\n",
    "def search_lc_scene(bbox, lc_date_range):\n",
    "    print(f\"Searching for Land Cover scenes within {lc_date_range} for bbox {bbox}\")\n",
    "    catalog = pystac_client.Client.open(\n",
    "        \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "        modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "    lc_search = catalog.search(\n",
    "        collections=[\"io-lulc-annual-v02\"],\n",
    "        bbox=bbox,\n",
    "        datetime=lc_date_range,\n",
    "    )\n",
    "    items = lc_search.item_collection()\n",
    "    print(f\"Found {len(items)} Land Cover scenes\")\n",
    "    return items\n",
    "\n",
    "def stack_s2_data(s2_items, s2_assets):\n",
    "    if not s2_items:\n",
    "        print(\"No Sentinel-2 items found.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(\"Stacking Sentinel-2 images...\")\n",
    "        stacked_data = stackstac.stack(\n",
    "            s2_items,\n",
    "            assets=s2_assets,\n",
    "            epsg=s2_items[0].properties[\"proj:epsg\"],\n",
    "            resolution=10,\n",
    "            bounds_latlon=s2_items[0].bbox,\n",
    "        ).median(\"time\", skipna=True).squeeze()\n",
    "\n",
    "        print(\"Stacked S2 data shape:\", stacked_data.shape)\n",
    "\n",
    "        if stacked_data is None or not isinstance(stacked_data, xr.DataArray):\n",
    "            print(\"Error: Stacking Sentinel-2 data resulted in an invalid output.\")\n",
    "            return None\n",
    "\n",
    "        return stacked_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Sentinel-2 data: {e}\")\n",
    "        return None\n",
    "\n",
    "def stack_lc_data(lc_items, s2_epsg):\n",
    "    if len(lc_items) == 0:\n",
    "        print(\"No Land Cover data found.\")\n",
    "        return None\n",
    "    try:\n",
    "        print(\"Stacking Land Cover images...\")\n",
    "\n",
    "        stacked_data = stackstac.stack(\n",
    "            lc_items,\n",
    "            dtype=np.ubyte,\n",
    "            fill_value=255,\n",
    "            sortby_date=False,\n",
    "            epsg=s2_epsg,\n",
    "            resolution=10,\n",
    "            bounds_latlon=lc_items[0].bbox\n",
    "        ).squeeze()\n",
    "\n",
    "        #Apply chunking after stacking\n",
    "        stacked_data = stacked_data.chunk({\"x\": 1024, \"y\": 1024})\n",
    "\n",
    "        print(\"Stacked LC data shape:\", stacked_data.shape)\n",
    "        print(f\"Chunk sizes: {stacked_data.chunks}\")\n",
    "\n",
    "        return stacked_data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error stacking Land Cover data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def unique_class(window, axis=None, **kwargs):\n",
    "    return np.all(window == window[:, :1, :, :1], axis=(1, 3))\n",
    "\n",
    "\n",
    "def is_homogeneous(lc_stack):\n",
    "    print(\"Checking for homogeneous LC regions...\")\n",
    "\n",
    "    # Ensure dimensions are exactly divisible by `sample_size`\n",
    "    height, width = lc_stack.shape[-2], lc_stack.shape[-1]\n",
    "    new_height = (height // sample_size) * sample_size\n",
    "    new_width = (width // sample_size) * sample_size\n",
    "\n",
    "    if new_height == 0 or new_width == 0:\n",
    "        print(\"Error: LC stack is empty after cropping. Skipping this AOI.\")\n",
    "        return None  \n",
    "\n",
    "    if new_height != height or new_width != width:\n",
    "        print(f\"Cropping LC stack to ({new_height}, {new_width}) to match sample_size.\")\n",
    "        lc_stack = lc_stack.isel(x=slice(0, new_height), y=slice(0, new_width))\n",
    "\n",
    "    # Drop mismatched coordinate variables\n",
    "    lc_stack = lc_stack.drop_vars([var for var in lc_stack.coords if \"x\" in var or \"y\" in var], errors=\"ignore\")\n",
    "\n",
    "    # Ensure rechunking aligns with the coarsening window\n",
    "    lc_stack = lc_stack.chunk({\"x\": sample_size, \"y\": sample_size})\n",
    "\n",
    "    # Check if lc_stack is empty after all modifications\n",
    "    if lc_stack.size == 0:\n",
    "        print(\"Error: LC stack is empty after cropping. Skipping this AOI.\")\n",
    "        return None  \n",
    "\n",
    "    # Apply coarsening and reduce with unique_class function\n",
    "    lc_uniqueness = lc_stack.coarsen(x=sample_size, y=sample_size, boundary=\"trim\").reduce(unique_class)\n",
    "    print(\"shape of lc_uniqueness:\", lc_uniqueness.shape)\n",
    "    print(\"datatype of lc_uniqueness:\", lc_uniqueness.dtype)\n",
    "    print(\"coordinates of lc_uniqueness:\", list(lc_uniqueness.coords))\n",
    "    print(\"type of lc_uniqueness:\", type(lc_uniqueness))\n",
    "    # Debugging: Print dtype of lc_uniqueness\n",
    "    print(f\"LC uniqueness dtype: {lc_uniqueness.dtype}, shape: {lc_uniqueness.shape}, type: {type(lc_uniqueness)}\")\n",
    "\n",
    "    return lc_uniqueness\n",
    "\n",
    "    \n",
    "def has_missing_values(array):\n",
    "    print(\"Checking for missing values...\")\n",
    "    return array.isnull().any().compute()\n",
    "\n",
    "def process_chips(aoi, s2_stack, lc_stack, output_dir, global_index):\n",
    "    print(f\"Processing chips for AOI at {aoi_test_bbox}\")\n",
    "\n",
    "    # Compute lc_uniqueness before looping\n",
    "    lc_uniqueness = is_homogeneous(lc_stack).compute()\n",
    "    if lc_uniqueness is None or lc_uniqueness.size == 0:\n",
    "        print(\"skipping this aoi, lc_uniqueness is size 0 or None\")\n",
    "        return global_index\n",
    "    print(\"shape of lc_uniqueness:\", lc_uniqueness.shape)\n",
    "    print(\"datatype of lc_uniqueness:\", lc_uniqueness.dtype)\n",
    "    print(\"coordinates of lc_uniqueness:\", list(lc_uniqueness.coords))\n",
    "    print(\"type of lc_uniqueness:\", type(lc_uniqueness))\n",
    "    \n",
    "    print(f\"LC uniqueness dtype after compute: {lc_uniqueness.dtype}, shape: {lc_uniqueness.shape}\")\n",
    "\n",
    "    for i in range(0, lc_stack.shape[1] - chip_size, sample_size):\n",
    "        for j in range(0, lc_stack.shape[2] - chip_size, sample_size):\n",
    "            \n",
    "            # Ensure indices stay within valid bounds\n",
    "            x_index = min(i // sample_size, lc_uniqueness.sizes[\"x\"] - 1)\n",
    "            y_index = min(j // sample_size, lc_uniqueness.sizes[\"y\"] - 1)\n",
    "\n",
    "            # Extract boolean value safely\n",
    "            value = lc_uniqueness.isel(x=x_index, y=y_index).values\n",
    "            print(f\"checking lc_uniqueness at ({x_index}, {y_index}): {value}, Type: {type(value)}, Shape: {value.shape}\")\n",
    "            if isinstance(value, np.ndarray) and value.size == 1:\n",
    "                value = value.item()\n",
    "\n",
    "            \n",
    "            if isinstance(value, np.ndarray):  \n",
    "                value = value.item()  # Ensure it's a single boolean value\n",
    "\n",
    "            if not value:\n",
    "                print(f\"Skipping chip at ({i}, {j}): LC region not homogeneous.\")\n",
    "                continue  \n",
    "\n",
    "            s2_chip = s2_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "            lc_chip = lc_stack.isel(x=slice(i, i + chip_size), y=slice(j, j + chip_size))\n",
    "\n",
    "            if has_missing_values(s2_chip):\n",
    "                continue  \n",
    "\n",
    "            chip_name = f\"s2_{date_range[:3]}_{global_index:05}.tif\"\n",
    "            lc_chip.rio.to_raster(os.path.join(output_dir, f\"lc_{chip_name}\"))\n",
    "            s2_chip.rio.to_raster(os.path.join(output_dir, chip_name))\n",
    "\n",
    "            print(f\"Saved chip {chip_name}\")\n",
    "            global_index += 1\n",
    "\n",
    "    return global_index\n",
    "\n",
    "# def crop_to_aoi(stack, bbox):\n",
    "#     \"\"\"Crop the Sentinel-2 stack to match the AOI bounding box\"\"\"\n",
    "#     minx, miny, maxx, maxy = bbox\n",
    "#     return stack.rio.clip_box(minx, miny, maxx, maxy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad2ce0-ca5d-4182-9e58-c071ab554609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39eb84a0-71bc-4f8e-bc98-1e92dd6e900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original AOI bounds: (31.20416267326229, 30.02337142235983, 31.280433633102717, 30.060612342021983)\n",
      "Reduced AOI bounds: [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023]\n",
      "Processing AOI 0 at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023]\n",
      "Searching for Sentinel-2 scenes for AOI at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023] within 2023-01-01/2023-03-31\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023] within 2023-04-01/2023-06-30\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023] within 2023-07-01/2023-09-30\n",
      "Found 1 Sentinel-2 scenes\n",
      "Searching for Sentinel-2 scenes for AOI at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023] within 2023-10-01/2023-12-31\n",
      "Found 1 Sentinel-2 scenes\n",
      "Stacking Sentinel-2 images...\n",
      "Stacked S2 data shape: (2, 11272, 11273)\n",
      "Error: s2_stack is missing CRS information. Assigning manually.\n",
      "Sentinel-2 CRS: EPSG:32636\n",
      "Searching for Land Cover scenes within 2023-01-01/2023-12-31 for bbox [30.91139663, 29.72604903, 32.06723105, 30.72965025]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 Land Cover scenes\n",
      "LC BBOX: [29.769589739624692, 23.97069843799003, 36.23041026037532, 32.035310832136304]\n",
      "AOI BBOX: [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023]\n",
      "Stacking Land Cover images...\n",
      "Stacked LC data shape: (2, 89417, 65754)\n",
      "Chunk sizes: ((1, 1), (1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 329), (1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 218))\n",
      "Processing chips for AOI at [31.230857509206437, 30.036405744241584, 31.25373879715857, 30.04757802014023]\n",
      "Checking for homogeneous LC regions...\n",
      "Cropping LC stack to (89344, 65536) to match sample_size.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/stackstac/prepare.py:408: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of lc_uniqueness: (2, 256, 256)\n",
      "datatype of lc_uniqueness: bool\n",
      "coordinates of lc_uniqueness: ['time', 'id', 'band', 'start_datetime', 'io:tile_id', 'end_datetime', 'proj:epsg', 'supercell', 'proj:shape', 'proj:transform', 'io:supercell_id', 'raster:bands', 'epsg']\n",
      "type of lc_uniqueness: <class 'xarray.core.dataarray.DataArray'>\n",
      "LC uniqueness dtype: bool, shape: (2, 256, 256), type: <class 'xarray.core.dataarray.DataArray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/gfm_bench/lib/python3.12/site-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 37.46 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of lc_uniqueness: (2, 256, 256)\n",
      "datatype of lc_uniqueness: bool\n",
      "coordinates of lc_uniqueness: ['time', 'id', 'band', 'start_datetime', 'io:tile_id', 'end_datetime', 'proj:epsg', 'supercell', 'proj:shape', 'proj:transform', 'io:supercell_id', 'raster:bands', 'epsg']\n",
      "type of lc_uniqueness: <class 'xarray.core.dataarray.DataArray'>\n",
      "LC uniqueness dtype after compute: bool, shape: (2, 256, 256)\n",
      "checking lc_uniqueness at (0, 0): [False False], Type: <class 'numpy.ndarray'>, Shape: (2,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Process chips\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m global_index, chip_dict \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_chips\u001b[49m\u001b[43m(\u001b[49m\u001b[43maoi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms2_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlc_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 169\u001b[0m, in \u001b[0;36mprocess_chips\u001b[0;34m(aoi, s2_stack, lc_stack, output_dir, global_index)\u001b[0m\n\u001b[1;32m    165\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):  \n\u001b[0;32m--> 169\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure it's a single boolean value\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping chip at (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m): LC region not homogeneous.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "## Main processing loop\n",
    "chip_dict = {}\n",
    "global_index = 0\n",
    "# Main processing loop\n",
    "for index, aoi in aoi_gdf.iterrows():\n",
    "    # Define the reduced AOI inside the loop\n",
    "    aoi_bounds = aoi.geometry.bounds  # Full AOI bounds\n",
    "    minx, miny, maxx, maxy = aoi_bounds\n",
    "\n",
    "    # Reduce size by a factor (adjustable)\n",
    "    shrink_factor = 0.3  # Reduce AOI size to 30%\n",
    "    \n",
    "    center_x = (minx + maxx) / 2\n",
    "    center_y = (miny + maxy) / 2\n",
    "\n",
    "    new_minx = center_x - (maxx - minx) * shrink_factor / 2\n",
    "    new_maxx = center_x + (maxx - minx) * shrink_factor / 2\n",
    "    new_miny = center_y - (maxy - miny) * shrink_factor / 2\n",
    "    new_maxy = center_y + (maxy - miny) * shrink_factor / 2\n",
    "\n",
    "    # Use the reduced AOI\n",
    "    aoi_test_bbox = [new_minx, new_miny, new_maxx, new_maxy]\n",
    "    \n",
    "    print(f\"Original AOI bounds: {aoi_bounds}\")\n",
    "    print(f\"Reduced AOI bounds: {aoi_test_bbox}\")\n",
    "    print(f\"Processing AOI {index} at {aoi_test_bbox}\")\n",
    "\n",
    "    # Search for Sentinel-2 images\n",
    "    s2_items = [search_s2_scenes(aoi, date_range) for date_range in s2_date_ranges]\n",
    "    \n",
    "    if any(len(items) == 0 for items in s2_items):\n",
    "        print(f\"Skipping AOI {index}: Missing Sentinel-2 data for at least one quarter.\")\n",
    "        continue\n",
    "\n",
    "    # Stack Sentinel-2 images\n",
    "    s2_stack = stack_s2_data([item[0] for item in s2_items if item], s2_bands)\n",
    "\n",
    "    # Ensure valid Sentinel-2 stack before proceeding\n",
    "    if s2_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid Sentinel-2 data found after cropping.\")\n",
    "        continue\n",
    "\n",
    "    if not hasattr(s2_stack, \"rio\") or s2_stack.rio.crs is None:\n",
    "        print(f\"Error: s2_stack is missing CRS information. Assigning manually.\")\n",
    "        s2_first_item = next((item[0] for item in s2_items if item), None)\n",
    "        if s2_first_item is None:\n",
    "            print(\"Error: No valid Sentinel-2 item found.\")\n",
    "            continue\n",
    "\n",
    "        s2_epsg = s2_first_item.properties[\"proj:epsg\"]\n",
    "        s2_stack.rio.write_crs(f\"epsg:{s2_epsg}\", inplace=True)\n",
    "\n",
    "    print(f\"Sentinel-2 CRS: {s2_stack.rio.crs}\")\n",
    "\n",
    "    # Search for Land Cover images\n",
    "    lc_items = search_lc_scene(s2_items[0][0].bbox, lc_year)\n",
    "    \n",
    "    if len(lc_items) == 0:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # Stack and process LC data\n",
    "    print(f\"LC BBOX: {lc_items[0].bbox}\")  # Debug LC item bbox\n",
    "    print(f\"AOI BBOX: {aoi_test_bbox}\")    # Debug AOI bbox\n",
    "    lc_stack = stack_lc_data(lc_items, s2_stack.rio.crs.to_epsg())\n",
    "\n",
    "    if lc_stack is None:\n",
    "        print(f\"Skipping AOI {index}: No valid LC data found.\")\n",
    "        continue\n",
    "\n",
    "    # Process chips\n",
    "    global_index, chip_dict = process_chips(aoi, s2_stack, lc_stack, output_dir, global_index)\n",
    "\n",
    "\n",
    "    if global_index > 5:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b0d4a-7f3a-435d-a784-f20a43a703ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional plotting code\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the first stored location\n",
    "if chip_dict:\n",
    "    first_location = next(iter(chip_dict.keys()))  # Get the first (i, j) key\n",
    "    first_four_chips = chip_dict[first_location]  # Get the corresponding chips\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(15, 6))  # 2 rows, 4 columns\n",
    "\n",
    "    for idx, (s2_chip, lc_chip) in enumerate(first_four_chips):\n",
    "        # Sentinel-2 RGB Visualization (assuming B4, B3, B2 are RGB)\n",
    "        rgb_image = s2_chip.sel(band=[3, 2, 1]).transpose(\"y\", \"x\", \"band\")  # Adjust band indices if needed\n",
    "        axes[0, idx].imshow(rgb_image.compute().clip(0, 3000) / 3000)  # Normalize for better visualization\n",
    "        axes[0, idx].set_title(f\"Sentinel-2 Chip {idx+1}\")\n",
    "        axes[0, idx].axis(\"off\")\n",
    "\n",
    "        # Land Cover Visualization\n",
    "        axes[1, idx].imshow(lc_chip.compute(), cmap=\"tab10\")  # Color map for categorical data\n",
    "        axes[1, idx].set_title(f\"Land Cover Chip {idx+1}\")\n",
    "        axes[1, idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbbc00-0965-493e-9567-1f0e556f6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chip_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb7a68-73e3-4c86-97dc-50ec125e3791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
